{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Science Project Week 3+4 on Network Dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emiel Steegh   - s1846388  \n",
    "Freek Nijweide - s1857746"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "For this week's assignments, we will examine [...]\n",
    "\n",
    "We use several packages this week,please make sure these are installed when trying to run our code:\n",
    "- networkx\n",
    "- numpy\n",
    "- plotly\n",
    "- pygraphviz\n",
    "\n",
    "We decided to use plotly instead of matplotlib, due to [...]\n",
    "\n",
    "its vastly superior 3D plotting capabilities, and its interactivity. We hoped that this would increase the legibility of our results, and we think that it has done so. We came to various conclusions for assignment 3 that we could not have reached without the assistance of interactive 3D plots.\n",
    "\n",
    "# TODO comment code\n",
    "\n",
    "# TODO mention dat er geen data gecollect is in de maand maart(?). Is duidelijk te zien aan plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The following includes are needed to work with graphs and display solutions.\n",
    "from __future__ import division\n",
    "import networkx as nx\n",
    "\n",
    "from IPython.display import SVG\n",
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "import StringIO\n",
    "\n",
    "from networkx.drawing.nx_pydot import read_dot\n",
    "from networkx.drawing.nx_pydot import from_pydot\n",
    "from networkx.drawing.nx_agraph import to_agraph\n",
    "import pydot\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import glob #used for filefinding\n",
    "import pprint #debugging purposes\n",
    "import pandas as pd #dataframes for plotly and .csv saving/reading\n",
    "import os #path handling\n",
    "import json\n",
    "\n",
    "import dateutil.parser\n",
    "\n",
    "try: #plotly does not come with the standard python and conda package\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.colors\n",
    "except:\n",
    "    print(\"\\n\"+\n",
    "          \"┌---------------- (!) Warning (!) ----------------┐\")\n",
    "    print(\"|                                                 |\")\n",
    "    print(\"| \\x1b[31m  It looks like Plotly could not be imported,\\x1b[0m   |\")\n",
    "    print(\"|   please make sure it is properly installed.    |\")\n",
    "    print(\"|                                                 |\")\n",
    "    print(\"└-------------------------------------------------┘\")\n",
    "else:\n",
    "    print(\"\\x1b[32mimports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#credits for the following function go to Benjamin Toueg\n",
    "#https://stackoverflow.com/questions/16888409/suppress-unicode-prefix-on-strings-when-using-pprint\n",
    "def my_safe_repr(object, context, maxlevels, level):\n",
    "    '''\n",
    "    when using prettyprint prints unicode as normal strings instead of u'(str)'\n",
    "    '''\n",
    "    typ = pprint._type(object)\n",
    "    if typ is unicode:\n",
    "        object = str(object)\n",
    "    return pprint._safe_repr(object, context, maxlevels, level)\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "pp.format = my_safe_repr\n",
    "\n",
    "def ANSI_col_code(color):\n",
    "    '''\n",
    "    changes a specified color or letter into it's ansii terminal color number counterpart\n",
    "    '''\n",
    "    # for color codes refer to \n",
    "    # https://en.wikipedia.org/wiki/ANSI_escape_code#Colors\n",
    "    code = 31\n",
    "    if type(color) not in [str, float]:\n",
    "        code = 31\n",
    "    elif color.lower() in ['r', \"red\"]:\n",
    "        code = 31\n",
    "    elif color.lower() in ['g', \"green\"]:\n",
    "        code = 32\n",
    "    elif color.lower() in ['b', \"blue\"]:\n",
    "        code = 94\n",
    "    elif color.lower() in ['y', \"yellow\"]:\n",
    "        code = 33\n",
    "    elif color.lower() in ['m', \"magenta\", 'p', \"purple\"]:\n",
    "        code = 35\n",
    "    return code\n",
    "\n",
    "def cprint(string, color = 'r'):\n",
    "    '''\n",
    "    prints a string with a color through the use of ansii escape characters\n",
    "    '''\n",
    "    code = ANSI_col_code(color)\n",
    "    print \"\\x1b[{}m{}\\x1b[0m\".format(code, string)\n",
    "\n",
    "def cstring(string, color = 'r'):\n",
    "    '''\n",
    "    returns a string with a color through the use of ansii escape characters\n",
    "    '''\n",
    "    code = ANSI_col_code(color)\n",
    "    return \"\\x1b[{}m{}\\x1b[0m\".format(code, string)\n",
    "\n",
    "cprint(\"Red\", 'r')\n",
    "cprint(\"Green\", 'g')\n",
    "cprint(\"Blue\", 'b')\n",
    "cprint(\"Yellow\", 'y')\n",
    "cprint(\"Purple\", 'm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data read, parse & store\n",
    "\n",
    "The code creates the necessary data from the many json files found in the supplied /data folders. <br>\n",
    "To save time the data is then stored in 4 seperate much smaller csv files which are read when the code is run. If they are not present, the .csv files will be created again.\n",
    "\n",
    "It should be noted that the datasets that rely on youtube (youtube_top100, radio538_alarmschijf & radio3fm_megahit) miss information between the 4th of January until the 21st of March, this can be clearly seen in most graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filetype_paths (data_folder, filetype='json'):\n",
    "    '''\n",
    "    returns a list of paths of the *.json files in a dir (data_folder)\n",
    "    '''\n",
    "    target = os.path.join(data_folder, '*.{}'.format(filetype))\n",
    "    paths = glob.glob(target)\n",
    "    return paths\n",
    "\n",
    "def path_to_timestamp(path):\n",
    "    '''\n",
    "    turns yyyymmdd into yyyy-mm-dd (the standard dateformat for plotly)\n",
    "    '''\n",
    "    dateraw = ((os.path.split(path)[1])[:8])\n",
    "    date = \"{}-{}-{}\".format(dateraw[:4], dateraw[4:6], dateraw[6:8])\n",
    "    return date\n",
    "\n",
    "\n",
    "def data_from_youtube(folder):\n",
    "    '''\n",
    "    read a supplied list of json files\n",
    "    the files contain a list of snapshots of data on a songs youtube page\n",
    "    from each entry in each file the following information is taken and put in one large dataframe\n",
    "    time of data collection | youtube name | youtube ID | release date | views | likes+dislikes | likes | dislikes\n",
    "    '''\n",
    "    \n",
    "    #grab the list of files to parse\n",
    "    ls_locations = filetype_paths(folder, 'json')\n",
    "    \n",
    "    #grab the list of files to parse\n",
    "    df = pd.DataFrame(columns=['timestamp', 'song', 'youtube_id',  'release_date', 'views', 'votes', 'likes', 'dislikes'])\n",
    "    \n",
    "    #dict conatining the different collumns for appending data\n",
    "    new_row = {'timestamp' : \"_\", 'release_date' : \"_\", 'song' : \"_\", 'youtube_id' : \"_\", 'views' : 0, 'votes' : 0, 'likes' : 0, 'dislikes' : 0}\n",
    "    \n",
    "    #loops through all files in the data folder\n",
    "    for loc in ls_locations:\n",
    "        \n",
    "        #grabs datagather timestamp from file title\n",
    "        new_row['timestamp'] = path_to_timestamp(loc)\n",
    "        file_raw_data =json.load(open(loc))\n",
    "        \n",
    "        #take all the other data from the relevant locations in the file\n",
    "        for entry in file_raw_data:\n",
    "            new_row['song'] = (entry[\"snippet\"])[\"title\"]\n",
    "            new_row['youtube_id'] = entry[\"id\"]\n",
    "            new_row['release_date'] = ((entry[\"snippet\"])[\"publishedAt\"])[:10]\n",
    "            stats = entry[\"statistics\"]\n",
    "            new_row['views'] = int(stats[\"viewCount\"])\n",
    "            new_row['likes'] = int(stats[\"likeCount\"])\n",
    "            new_row['dislikes'] = int(stats[\"dislikeCount\"])\n",
    "            new_row['votes'] = new_row['likes']+new_row['dislikes']\n",
    "            \n",
    "            df = df.append(new_row, ignore_index=True)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def data_from_spotify(folder):\n",
    "    '''\n",
    "    read a supplied list of json files\n",
    "    the files contain a list of the top songs on spotify that day with their respective data\n",
    "    from each entry in each file the following information is taken and put in one large dataframe\n",
    "    time of data collection | spotify name | spotify ID | release date | popularity rank | duration in ms\n",
    "    '''\n",
    "    \n",
    "    #grab the list of files to parse    \n",
    "    ls_locations = filetype_paths(folder, 'json')\n",
    "    \n",
    "    #grab the list of files to parse\n",
    "    df = pd.DataFrame(columns = ['timestamp', 'song', 'spotify_id', 'release_date', 'popularity','duration_ms'])\n",
    "    \n",
    "    #dict conatining the different collumns for appending data\n",
    "    new_row = {'timestamp' : \"_\", 'release_date' : \"_\", 'song' : \"_\", 'spotify_id' : \"_\", 'popularity' : 0, 'duration_ms':0}\n",
    "    \n",
    "    #loops through all files in the data folder\n",
    "    for loc in ls_locations:\n",
    "        \n",
    "        #grabs datagather timestamp from file title\n",
    "        new_row['timestamp'] = path_to_timestamp(loc)\n",
    "        file_raw_data = json.load(open(loc))\n",
    "        \n",
    "        #take all the other data from the relevant locations in the file\n",
    "        items = (file_raw_data[\"tracks\"])[\"items\"] # the list of song-datas in list format\n",
    "        for item in items:\n",
    "            track = item[\"track\"]\n",
    "            \n",
    "            #some songs have many artists, anyone except the first are usually listed in the title as featuring artist\n",
    "            #so this part may no be necessary, only taking the first artist and neglecting the rest may work neater\n",
    "            #but we are not certain that all artists will be mentioned under featuring, so we list many double names but none will be excluded for sure\n",
    "            artists = \"\"\n",
    "            for artist in (track[\"artists\"]):\n",
    "                add_artist = (artist[\"name\"]).encode('utf-8')\n",
    "                if artists == \"\":\n",
    "                    artists = add_artist\n",
    "                else:\n",
    "                    artists += \", {}\".format(add_artist)\n",
    "                    \n",
    "            new_row['song'] = \"{} - {}\".format(artists, (track[\"name\"]).encode('utf-8'))\n",
    "            new_row['spotify_id'] = track[\"id\"]\n",
    "            new_row['popularity'] = track[\"popularity\"]\n",
    "            new_row['duration_ms'] = track['duration_ms']\n",
    "            new_row['release_date'] = (item[\"added_at\"])[:10]            \n",
    "            \n",
    "            df = df.append(new_row, ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "            \n",
    "    \n",
    "def create_and_store_data():\n",
    "    '''\n",
    "    please only run this through def check_or_create_data\n",
    "    \n",
    "    Wrapper of the read and parse function, stores the desired data from the 4 folders in 4 seperate .csv files\n",
    "    The reading all the data files and wrangling the data into a useful format takes a bunch of your time, we dont want that.\n",
    "    This definition does all that once and stores the created dataframes as csv files in a new folder.\n",
    "    This way the process only has to happen once and on all consecutive runs of the notebook, we just have to read 4 files that contain the data we want and in the way we want it\n",
    "    '''\n",
    "    \n",
    "    location = os.path.join('data','spotify_top100') #spotify top 100\n",
    "    df_spotify_top100 = data_from_spotify(location)\n",
    "    df_spotify_top100.to_csv(os.path.join('csv_data', 'wrangled_spotify_top100.csv'), index=False, sep='\\t', encoding='utf-16')\n",
    "    cprint(\"set (1/4) done: spotify\",'y')\n",
    "    #pp.pprint(df_spotify_top100)\n",
    "\n",
    "    location = os.path.join('data','youtube_top100') #youtube top 100\n",
    "    df_youtube_top100 = data_from_youtube(location)\n",
    "    df_youtube_top100.to_csv(os.path.join('csv_data', 'wrangled_youtube_top100.csv'), index=False, sep='\\t', encoding='utf-16')\n",
    "    cprint(\"set (2/4) done: youtube\",'y')\n",
    "    #pp.pprint(df_youtube_top100)\n",
    "\n",
    "    location = os.path.join('data','radio3fm_megahit') #radio 3fm popular track of the day\n",
    "    df_3fm_megahit = data_from_youtube(location)\n",
    "    df_3fm_megahit.to_csv(os.path.join('csv_data', 'wrangled_3fm_megahit.csv'), index=False, sep='\\t', encoding='utf-16' )    \n",
    "    cprint(\"set (3/4) done: radio 3fm\",'y')\n",
    "    #pp.pprint(df_3fm_megahit)\n",
    "\n",
    "    location = os.path.join('data','radio538_alarmschijf') #radio 538 tracks that may become popular??\n",
    "    df_538_alarmschijf  = data_from_youtube(location)\n",
    "    df_538_alarmschijf.to_csv(os.path.join('csv_data', 'wrangled_538_alarmschijf.csv'), index=False, sep='\\t', encoding='utf-16')    \n",
    "    cprint(\"set (4/4) done: radio 538\",'y')\n",
    "    #pp.pprint(df_538_alarmschijf)\n",
    "\n",
    "#these variables need to be global    \n",
    "data_megahit      = 'global'\n",
    "data_alarmschijf  = 'global'\n",
    "data_youtube100   = 'global'\n",
    "data_spotify100   = 'global'\n",
    "\n",
    "\n",
    "def check_or_create_data():\n",
    "    '''\n",
    "    Checks wether the required data files are present, if they are not runs the func to create them\n",
    "    does NOT check for the files individually\n",
    "    does NOT check the file content for changes or corruptions\n",
    "    WILL recreate all the files if a single one is missing\n",
    "    '''\n",
    "    \n",
    "    #creat the foler to store them in\n",
    "    if not os.path.exists('csv_data'):\n",
    "        os.mkdir('csv_data')\n",
    "    \n",
    "    csv_goal = [os.path.join('csv_data', 'wrangled_3fm_megahit.csv'),\n",
    "               os.path.join('csv_data', 'wrangled_538_alarmschijf.csv'),\n",
    "               os.path.join('csv_data', 'wrangled_youtube_top100.csv'),\n",
    "               os.path.join('csv_data', 'wrangled_spotify_top100.csv')]\n",
    "    csv_present = filetype_paths('csv_data', 'csv')\n",
    "    \n",
    "    #checks the if the desired file names\n",
    "    if sorted(csv_present) == sorted(csv_goal):\n",
    "        cprint(\"all data files seem present!\",'g')\n",
    "    else:\n",
    "        cprint(\"data is missing and will be created, this may take a while\", 'r')\n",
    "        create_and_store_data()\n",
    "        cprint(\"all done!\", 'g')\n",
    "        \n",
    "    #change the global versions of the variables\n",
    "    global data_megahit, data_alarmschijf, data_youtube100, data_spotify100\n",
    "    data_megahit = pd.read_csv(os.path.join('csv_data', 'wrangled_3fm_megahit.csv'), delimiter = '\\t', encoding='utf-16')\n",
    "    data_alarmschijf = pd.read_csv(os.path.join('csv_data', 'wrangled_538_alarmschijf.csv'), delimiter = '\\t', encoding='utf-16')\n",
    "    data_youtube100 = pd.read_csv(os.path.join('csv_data', 'wrangled_youtube_top100.csv'), delimiter = '\\t', encoding='utf-16')\n",
    "    data_spotify100 = pd.read_csv(os.path.join('csv_data', 'wrangled_spotify_top100.csv'), delimiter = '\\t', encoding='utf-16')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "check_or_create_data()\n",
    "\n",
    "testprint = False\n",
    "if testprint:\n",
    "    print \"\\n\\n\"\n",
    "    cprint(\"--- --- Radio3fm Megahit:\",'p')\n",
    "    print data_megahit\n",
    "    cprint(\"--- --- Radio538 Alarmschijf:\",'p')\n",
    "    print data_alarmschijf\n",
    "    cprint(\"--- --- Spotify Top100 Dataset:\",'p')\n",
    "    print data_spotify100\n",
    "    cprint(\"--- --- Youtube Top100 Dataset:\",'p')\n",
    "    print data_youtube100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1 - Cascading effects\n",
    "\n",
    "*Read the material in Chapter 16 on cascading effects. Plot the difference between the number of likes and dislikes for several songs.*\n",
    "\n",
    "*Do you think we observe cascading effects? Is there a difference between songs that are already popular (in the top-100) and those that are not (megahit or alarmschijf)?. The model in Chapter 16 in its pure form cannot be applied to music preferences. Why not? Can you modify the model accordingly? Can you quantify cascading effects in this setting?*\n",
    "\n",
    "*There is no one right answer right to this question, and the answer may depend on a particular song. Make your conclusions based on your data and present clear arguments.*\n",
    "\n",
    "# TODO @Emiel leg uit verschil tussen top 100 songs plot en niet top 100 (megahit, alarmschijf). Worden deze al geplot? Zo niet, even fixen. Is weinig werk, zie bestaande code. Er staat in de exercise dat dit moet.\n",
    "\n",
    "# TODO @Emiel explain drake jumpman. Nummer met laag aantal views, duidelijke cascading, yadda yadda.\n",
    "\n",
    "\n",
    "# TODO @Emiel why cant the ch 16 model bot be applied to songs (int vs bool)\n",
    "\n",
    "# TODO @Emiel quantify cascading effect\n",
    "\n",
    "# TODO @Emiel. Maybe rename find_songs want het vindt helemaal geen songs (gebruik pycharm hiervoor, refactor tool). Doe hetzelfde voor de variabele \"y_id\" ? \n",
    "\n",
    "# TODO @Emiel explain wat we uberhaupt doen (verschil tussen relative likes en like ratio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def find_songs(dataframe,mode='relative_likes'):\n",
    "    \"\"\"\n",
    "    Used to convert a DataFrame containing many songs into a nice dictionary that contains data per songs.\n",
    "    Useful for when we want to plot data\n",
    "    \"\"\"\n",
    "    dt_songs = {}\n",
    "    for index, row in sorted(dataframe.iterrows(), key=lambda k: k[1]['timestamp']): # Sort data by timestamp before doing \n",
    "                                                                                     # anything to avoid weird errors\n",
    "        y_id = row['song'] # Get song name\n",
    "        \n",
    "        # For different modes, the data plotted will be different\n",
    "        if mode=='relative_likes': \n",
    "            data = int(row['likes'])-int(row['dislikes'])\n",
    "        elif mode=='like_ratio':\n",
    "            data = int(row['likes'])/int(row['votes'])\n",
    "        elif mode=='expectation':\n",
    "            data = int(row['views'])\n",
    "        else:\n",
    "            # Just use the mode directly. For example, using mode='likes' will make the like count the data\n",
    "            data = int(row[mode])\n",
    "            \n",
    "        \n",
    "        if mode=='expectation': # When plotting expectation (later on), use likes for x axis. Otherwise, we always use timestamp\n",
    "            data2 = int(row['likes'])\n",
    "        else:\n",
    "            data2 = row['timestamp']\n",
    "        \n",
    "        if y_id not in dt_songs.keys(): # If this song is not in the dict yet, add this entry containing 2 lists\n",
    "            dt_songs[y_id] = [[data],[data2]]\n",
    "        else: # If the song is already in the dict, just append the data to the existing lists\n",
    "            dt_songs[y_id][0].append(data)\n",
    "            dt_songs[y_id][1].append(data2)\n",
    "                        \n",
    "    return(dt_songs)\n",
    "     \n",
    "\n",
    "\n",
    "def draw_linechart(figure,dt_data,plot_trendline=False,**kwargs):\n",
    "    \"\"\"\n",
    "    Takes data as generated by the find_songs function, and plots the data per song\n",
    "    \"\"\"\n",
    "    colors=plotly.colors.DEFAULT_PLOTLY_COLORS # List of colors\n",
    "    i=0    \n",
    "    if plot_trendline: \n",
    "        # If we plot the trendline, calculate the a list of all data points on the x-axis for all songs\n",
    "        all_x = [data[1] for song, data in dt_data.items()]\n",
    "        flattened_x = [item for sublist in all_x for item in sublist]\n",
    "    \n",
    "    for song, data in dt_data.items():\n",
    "        color = colors[i] # Use the same color for both trendline and data\n",
    "        \n",
    "        # Set x, y, and name of the data in legend\n",
    "        kwargs['y'] = data[0]\n",
    "        kwargs['x'] = data[1]\n",
    "        kwargs['name']=song\n",
    "        \n",
    "        # Set default value for mode, line, marker (plotly settings)\n",
    "        mode='markers+lines'\n",
    "        line={'width':0.5,'color':color}\n",
    "        marker={'size':3,'color':color}\n",
    "        \n",
    "        # Try getting variables from the arguments passed to this method, and override the defaults defined above\n",
    "        if 'mode' in kwargs.keys():\n",
    "            mode = kwargs['mode']\n",
    "        if 'line' in kwargs.keys():\n",
    "            line = kwargs['line']\n",
    "        if 'marker' in kwargs.keys():\n",
    "            marker = kwargs['marker']\n",
    "        \n",
    "        # Plot the data\n",
    "        figure.add_trace(go.Scatter(kwargs,mode=mode,line=line,marker=marker))\n",
    "        \n",
    "        # Plot the trendline (if needed)\n",
    "        if plot_trendline:\n",
    "            trendline = np.poly1d(np.polyfit(kwargs['x'],kwargs['y'],1))\n",
    "            figure.add_trace(go.Scatter(x=flattened_x,y=trendline(flattened_x),mode='lines',name='Trendline for' + song,line={'width':0.3,'color':color,'dash':'dot'}))\n",
    "        \n",
    "        # Increment i, and set it back to 0 if we are at end of list. (Used to correctly select colors)\n",
    "        i = (i + 1) % (len(colors)-1 ) \n",
    "        \n",
    "    \n",
    "def get_n_songs(data,n=None,mode='sample'):\n",
    "    \"\"\"\n",
    "    Gets data for ~n songs (might be less due to duplicates)\n",
    "    \"\"\"\n",
    "    if n:\n",
    "        if mode=='sample': # Return random sample\n",
    "            return data[data.song.isin(data.song.sample(n=n))]\n",
    "        elif mode=='head': # Return from start of dataframe\n",
    "            return data[data.song.isin(data.song.head(n=n))]\n",
    "        elif mode=='tail': # Return from end of dataframe\n",
    "            return data[data.song.isin(data.song.tail(n=n))]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode\")\n",
    "            \n",
    "    else: # If n is None, just return the entire dataframe (sample = population)\n",
    "        return data\n",
    "\n",
    "# Make figures\n",
    "figure = go.Figure()\n",
    "figure2 = go.Figure()\n",
    "    \n",
    "# Take 10 random youtube songs, \n",
    "data_set = pd.concat([get_n_songs(data_youtube100,n=5), data_alarmschijf] )\n",
    "\n",
    "data1 = find_songs(data_set,mode='relative_likes') # Get relative likes of data in nice format\n",
    "draw_linechart(figure,data1,mode='markers+lines',line={'width':0.5},marker={'size':3}) # Plot it\n",
    "\n",
    "data2 = find_songs(data_set,mode='like_ratio') # Get like ratio of data in nice format\n",
    "draw_linechart(figure2,data2) # Plot it\n",
    "\n",
    "# Update layout, title etc. of figures\n",
    "\n",
    "figure.update_layout(\n",
    "    legend=dict(x=0, y=-2),\n",
    "    title='Likes minus dislikes of several youtube songs over time',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Likes minus dislikes')\n",
    "\n",
    "figure2.update_layout(\n",
    "    legend=dict(x=0, y=-2),\n",
    "    title='Like ratio (in terms of total votes) for several youtube songs over time',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Like ratio')\n",
    "\n",
    "# Show figures\n",
    "\n",
    "figure.show()\n",
    "figure2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the question wether a cascading effect is present we instead ask ourselves:<br>\n",
    "\"Is a listeners choice to like or dislike influenced by the previous likes or dislikes?\"\n",
    "\n",
    "all of the top100 songs already existed or were already popular at the first point of measurement, so it is difficult to say something about the start of the cascade. We are in luck however as the song Jumpman by Drake and Soulja Boy is an accident (we assume) in the dataset. There is a song Jumpman by Drake featuring Future, and a seperate song Jumpman by Souljaboy. The data for the youtube video however has some intersting behaviour. It starts with almost no views and as time progresses the ratio of likes stabilizes\n",
    "\n",
    "| notation |explanation| value |\n",
    "|:-----|:----------|:-------|\n",
    "| G | song is actually good | P[G] = p |\n",
    "| B | song is actually bad | P[B] = 1-p | \n",
    "| H | private high signal (listener enjoys) | P[H\\|G] = q |\n",
    "| L | private low signal (listener does not enjoy) | P[L\\|G] = 1-q |\n",
    "| a | high signals of previous people | amount of likes  |\n",
    "| b | low signals of previous people  | amount of dislikes |\n",
    "| v<sub>g</sub> | payoff when accepting a good state | ? |\n",
    "| v<sub>b</sub> | payoff when rejecting a bad state | ? |\n",
    "\n",
    "\n",
    "We have no way of telling objectively wether the sitution is G or B. The songs have no objective quality trait and music is very taste based.\n",
    "\n",
    "The payoff could be an improved or worsened recommendation algorithm, where a user gets more or less songs they actually like.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Read the material in Chapter 16 on cascading effects. Plot the difference between the number of likes and dislikes for several songs.\n",
    "\n",
    "Do you think we observe cascading effects? Is there a difference between songs that are already popular (in the top-100) and those that are not (megahit or alarmschijf)?. The model in Chapter 16 in its pure form cannot be applied to music preferences. Why not? Can you modify the model accordingly? Can you quantify cascading effects in this setting?\n",
    "\n",
    "There is no one right answer right to this question, and the answer may depend on a particular song. Make your conclusions based on your data and present clear arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Network effects\n",
    "\n",
    "*Study the material in Chapter 17. Our goal now is to investigate whether we observe network effects in music preferences. How will you choose the data for this purpose? Which songs will suit most?*\n",
    "\n",
    "The songs that will suit most are ones that we can track their rise and fall of popularity of from the moment they were published. The most suitable candidates are those that have been a 3FM megahit or on the 538 alarmschijf, as they were quite unknown when being published on there. To compare, we will choose some songs from the Youtube and Spotify top 100 list. For these songs, the popularity / viewcount is the most interesting data to plot over time.\n",
    "\n",
    "*For several songs of your choice plot the actual number of views against time. Assume that without network effects we can expect that users visit a website with a certain frequency and view the song if it matches their taste. Hence, the expected number of views grows linearly in time. Compare your plot to Figure 17.4. Do you think you observe network effects?*\n",
    "\n",
    "\n",
    "We plot the viewcount of the Megahit / Alarmschijf songs in the first plot, and the viewcount + popularity of some random top 100 YouTube / Spotify songs in the second plot.\n",
    "\n",
    "In the first plot, Alarmschijf songs have a dotted line, while Megahit songs have an opaque line.\n",
    "\n",
    "In the second plot, the Youtube songs have a dashed line. Their y-axis is the view count, as seen on the left side of the plot. The Spotify songs have an opaque line, and their y-axis is \"popularity\" (a number calculated using some algorithm, rannging from 0 to 100), as seen on the right side of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Grab the data\n",
    "some_youtube_songs = data_youtube100[data_youtube100.song.isin(data_youtube100.song.head(n=5))]  \n",
    "some_spotify_songs = data_spotify100[data_spotify100.song.isin(data_spotify100.song.head(n=5))]  \n",
    "\n",
    "# Make figures\n",
    "unknown_songs_figure = go.Figure()\n",
    "youtube_songs_figure = go.Figure()\n",
    "\n",
    "# Plot alarmschijf data\n",
    "alarmschijf_data = find_songs(data_alarmschijf,mode='views')\n",
    "draw_linechart(unknown_songs_figure,alarmschijf_data,mode='lines',line={'dash':'dot'},legendgroup='alarmschijf')\n",
    "\n",
    "# Plot megahit data\n",
    "megahit_data = find_songs(data_megahit,mode='views')\n",
    "draw_linechart(unknown_songs_figure,megahit_data,mode='lines',legendgroup='megahit')\n",
    "\n",
    "# Plot youtube top100 data\n",
    "some_youtube_songs_data = find_songs(some_youtube_songs,mode='views')\n",
    "draw_linechart(youtube_songs_figure,some_youtube_songs_data,mode='lines',line={'dash':'dot'},legendgroup='some_youtube_songs')\n",
    "\n",
    "# Plot spotify top100 data\n",
    "some_spotify_songs_data = find_songs(some_spotify_songs,mode='popularity')\n",
    "draw_linechart(youtube_songs_figure,some_spotify_songs_data,plot_trendline=False,yaxis='y2',legendgroup='some_spotify_songs')\n",
    "\n",
    "\n",
    "### Update plot titles and such\n",
    "\n",
    "unknown_songs_figure.update_layout(\n",
    "    title='Views of \"new\" songs from Megahit/Alarmschijf over time',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='View count')\n",
    "\n",
    "youtube_songs_figure.update_layout(    \n",
    "    title='Views/popularity of popular Youtube and Spotify songs over time',\n",
    "    xaxis_title='Date',\n",
    "    yaxis=dict(title=\"Youtube views\")\n",
    "    \n",
    "    ,yaxis2=dict(\n",
    "        title=\"Spotify popularity\",\n",
    "        anchor=\"free\",\n",
    "        overlaying=\"y\",\n",
    "        side=\"right\",\n",
    "        position=1\n",
    "    ),\n",
    "    legend=dict(x=0, y=-2)\n",
    ")\n",
    "\n",
    "# Show plots\n",
    "\n",
    "unknown_songs_figure.show()\n",
    "youtube_songs_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot explanation\n",
    "\n",
    "The 3FM Megahit and 538 Alarmschijf songs (first plot) all seem to have quite a linear relationship between time and popularity, while the more popular songs from the top 100 (second plot) have a relationship between time and popularity which is very similar to figure 17.4 (a fast rise at first, then a more slow stabilization towards the equilbrium). The data from Spotify seems to be much more extreme in this than the data from YouTube. We do not know what causes this, as Spotify does not release the way that they calculate this \"popularity\" ranking, which is a number from 0 to 100, to the public. It may simply be the percentile the song is in, when sorting all songs by the amount of plays in one week. All we know is that it [is not updated in real-time, and may take a few days to be updated](https://community.spotify.com/t5/Content-Questions/Artist-popularity/m-p/4734566/highlight/true#M32231).\n",
    "\n",
    "Thus, we can see quite clearly that network effects are not visible for impopular songs (users simply come across it by visiting a platform and clicking songs that match their preferences, causing a linear growth in views), while they are visible for more popular songs (people listen to them because of their popularity, causing a growth like in figure 17.4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2 (continued)\n",
    "\n",
    "*How will you interpret the network benefit function f, the intrinsic interest function r, and the price p*? Try to specify the model that best fits the data.*\n",
    "\n",
    "# TODO @Emiel as explained before, we can't model price, is gewoon 0 of 1.\n",
    "\n",
    "For plots like for functions f and r, normally the share of consumers that buys something is plotted on the x axis, and the y-axis represents the \"price\" of the good. There are multiple interpretations possible for this, and we will explore all of them.\n",
    "\n",
    "In the first case, the share of consumers can be represented by the amount of views (amount of people that took the time to click on the video). Then, what do we plot on the y-axis? The amount of likes is not interesting to put on the y-axis, as there is a very strong (yet uninteresting and trivial) correlation between the amount of likes and the amount of views, which will tell us nothing about network benefits or intrinsic interest.\n",
    "We chose to plot two metrics for the price: the like ratio, $\\frac{likes}{total\\ votes}$ and the likes per view. Both are good estimators for the true price, as they show the share of people who think that paying the price of clicking the video was worth it.\n",
    "This can be seen in the first plot. The likes per view and like ratio use separate y axes because, while they use the same data for the x-axis, the scale of their y-axis is completely different. The y-axis for likes per view can be seen on the left side, and the y-axis for like ratio can be seen on the right side.\n",
    "\n",
    "For the second case, the share of consumers willing to \"pay\" for the music can be represented by the like ratio. If people dislike a song, they didn't think that spending the time to click on it and start listening is worth it. Therefore, the like ratio is the true share of people that think the price of clicking on this song is worth it. Then how do we model the price? In this case, there are two possibilities. The first one is the amount of views being a way to represent the price: that amount of people were willing to pay the true price of clicking the video. We think that this approach dos not make any sense in retrospect, but the plot was left in to show the correlation. The y-axis for this is on the left side of hte plot.\n",
    "Another approach would be to plot the likes per view on the y-axis. This represents the engagement rate of the video. The y-axis for this is on the right side of the plot.\n",
    "\n",
    "We think that the first case is more likely to be correct, as it is quite hard to make good arguments for the second case. We only discovered this after already plotting this data during the exploratory phase, and left it in for the sake of showing our work.\n",
    "\n",
    "A third possible way to plot the data uses Spotify data. Here, the song's max popularity over all time is the share of the population willing to \"buy\" the product, while the \"price\" they pay is the duration of the song in milliseconds. This is seen in the third plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Gathering data\n",
    "\n",
    "merged_data = pd.concat([data_youtube100,data_alarmschijf,data_megahit]) #concatenate youtube 100, alarmschijf, and megahit data\n",
    "data = merged_data[merged_data.timestamp == u'2016-11-28'] # Plot data on november 28 (last date at which data was collected for all)\n",
    "\n",
    "dislikes_over_votes = (data.dislikes/data.votes)\n",
    "likes_over_votes = data.likes/data.votes\n",
    "likes_over_dislikes = data.likes/data.dislikes\n",
    "\n",
    "### Plot 1\n",
    "figure = go.Figure()\n",
    "\n",
    "# Plot likes per view and trendline\n",
    "figure.add_trace(go.Scatter(x=data.views,y=data.likes/data.views,mode='markers',name='Likes per view',marker={'color':'royalblue'}))\n",
    "trendline1 = np.poly1d(np.polyfit(data.views,data.likes/data.views,1))\n",
    "figure.add_trace(go.Scatter(x=data.views,y=trendline1(data.views),mode='lines',name='Likes per view trendline',line={'color':'royalblue'}))\n",
    "\n",
    "# Plot like ratio and trendline\n",
    "figure.add_trace(go.Scatter(x=data.views,y=likes_over_votes,mode='markers',name='Like ratio',yaxis='y2',marker={'color':'tomato'}))\n",
    "trendline2 = np.poly1d(np.polyfit(data.views,likes_over_votes,1))\n",
    "figure.add_trace(go.Scatter(x=data.views,y=trendline2(data.views),mode='lines',name='Like ratio trendline',line={'color':'tomato'},yaxis='y2'))\n",
    "\n",
    "### Plot 2\n",
    "figure2 = go.Figure()\n",
    "\n",
    "# Plot views\n",
    "figure2.add_trace(go.Scatter(x=likes_over_votes,y=data.views,mode='markers',name='Views',marker={'color':'royalblue'}))\n",
    "trendline1 = np.poly1d(np.polyfit(likes_over_votes,data.views,1))\n",
    "figure2.add_trace(go.Scatter(x=likes_over_votes,y=trendline1(likes_over_votes),mode='lines',name='View count trendline',line={'color':'royalblue'}))\n",
    "\n",
    "# Plot likes per view\n",
    "figure2.add_trace(go.Scatter(x=likes_over_votes,y=data.likes/data.views,mode='markers',name='Likes per view',yaxis='y2',marker={'color':'tomato'}))\n",
    "trendline2 = np.poly1d(np.polyfit(likes_over_votes,np.array(data.likes/data.views),1))\n",
    "figure2.add_trace(go.Scatter(x=likes_over_votes,y=trendline2(likes_over_votes),mode='lines',name='Likes per view trendline',yaxis='y2',line={'color':'tomato'}))\n",
    "\n",
    "### Plot 3\n",
    "figure3 = go.Figure()\n",
    "# Plotting duration against sum of all instances of song's popularity (discrete version of integral of popularity over time)\n",
    "\n",
    "# Get new dataset, from spotify\n",
    "data = get_n_songs(data_spotify100,n=None)\n",
    "\n",
    "# Get the max popularity of each song\n",
    "max_popularities=( [ (data[data.spotify_id == spotify_id].duration_ms.iloc[0] , sum(list(data[data.spotify_id == spotify_id].popularity)) ) for spotify_id in data.spotify_id.unique()])\n",
    "\n",
    "# Make this into two lists, for plotting purposes\n",
    "ls_durations, ls_max_popularities = map(list, zip(*max_popularities))\n",
    "\n",
    "#Only use data from last date at which data was collected\n",
    "data = data[data.timestamp == u'2015-12-15']\n",
    "\n",
    "# Plot duration vs popularity and trendline\n",
    "figure3.add_trace(go.Scatter(x=ls_max_popularities,y=ls_durations,mode='markers',name='Song duration',marker={'color':'royalblue'}))\n",
    "trendline1 = np.poly1d(np.polyfit(ls_max_popularities,ls_durations,1))\n",
    "figure3.add_trace(go.Scatter(x=ls_max_popularities,y=trendline1(ls_max_popularities),mode='lines',name='Song duration trendline',line={'color':'royalblue'}))\n",
    "\n",
    "\n",
    "\n",
    "### Updating figures (titles, axis titles etc)\n",
    "\n",
    "figure.update_layout(    \n",
    "    yaxis=dict(title=\"Likes per view\"),\n",
    "    title='Like ratio and likes per view vs amount of views',\n",
    "    xaxis_title='View count',\n",
    "    yaxis2=dict(\n",
    "        title=\"Like ratio\",\n",
    "        anchor=\"free\",\n",
    "        overlaying=\"y\",\n",
    "        side=\"right\",\n",
    "        position=1\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "figure2.update_layout(\n",
    "    title='Amount of views, and likes per view vs like ratio',\n",
    "    yaxis=dict(title=\"Views\"),\n",
    "    xaxis_title='Like ratio',\n",
    "    yaxis2=dict(\n",
    "        title=\"Likes per view\",\n",
    "        anchor=\"free\",\n",
    "        overlaying=\"y\",\n",
    "        side=\"right\",\n",
    "        position=1\n",
    "    ),\n",
    ")\n",
    "\n",
    "figure3.update_layout(\n",
    "    title='Song duration vs song popularity',\n",
    "    xaxis_title=\"Maximum of a song's popularity (integer in range [0,100])\",\n",
    "    yaxis_title='Song duration (ms)'\n",
    ")\n",
    "\n",
    "# Show the plots\n",
    "\n",
    "figure.show()\n",
    "figure2.show()\n",
    "figure3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in plot 1, there is an obvious negative correlation between the amount of views, and the like ratio or likes per view. This gives us a figure similar to figure 17.2. More popular videos seem to lead to less user interaction, and attract more dislikes (which could be due to internet trolls, haters, and people disliking something as it becomes too popular). Because the amount of likes per view does not suffer from this problem, we think this is the most interesting way to model the price in a price-consumersplot. When the amount of people that have \"consumed\" media of high quality is low, they are likely to want to spread it to friends, and press the like button. Thus, they think the 'price' of the video was too low, and more people should click the video.\n",
    "\n",
    "It is less easy to obtain meaningful conclusions from the second plot. For the amount of likes, we can come to the same conclusion as we did one paragraph above. The observation that there is a positive relationship between like ratio and likes per view is interesting, but this is not a proper model for customer interest or network effects.\n",
    "\n",
    "The third plot is also not as helpful as we hoped. It is obvious that songs cluster around being 210k ms in length, and thus the most popular songs are also around this. However, song popularity has no influence on the duration (which makes sense, as duration is an independent variable which can be changed by the artist, while popularity is not). This plot would have been better with the x-axis and the y-axis swapped, but that could not have been a potential model for price vs. consumer interest.\n",
    "\n",
    "Thus, we were able to find a suitable model for r(x): the likes per view vs the view count.\n",
    "The model for the price is then: the amount of likes per view (which could be seen as an approximation of a video's \"true quality\", therefore being directly proportional to the \"price\" people are willing to pay).\n",
    "\n",
    "We were unable to find a suitable model for f(x) (the benefit of the good when x amount of the population use it). This is supposed to have a positive correlation, and the only one we found so far is the correlation between the like ratio and the amount of likes per view. But this does not seem like a good approach (the amount of likes per view has a trivial correlation with the like ratio). We will continue looking for a way to model this below.\n",
    "\n",
    "We will plot the amount of views against the amount of likes, hoping to find something like figure 17.9 (outcome vs expectations). We will use two plots: one with a trendline per song, and one with only one trendline, for all songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "songs=get_n_songs(data_youtube100,n=6,mode='sample') # Get n random songs\n",
    "\n",
    "data1 = find_songs(songs,mode='expectation') # X axis will be likes, Y axis will be views\n",
    "figure = go.Figure()\n",
    "draw_linechart(figure,data1,plot_trendline=True,mode='markers') # plot this data, and the trendline for each song\n",
    "figure.update_layout(\n",
    "    showlegend=False,\n",
    "    title='Views vs likes for several songs (trendline per song)',\n",
    "    xaxis_title='Likes',\n",
    "    yaxis_title='views'\n",
    ")\n",
    "figure.show()\n",
    "\n",
    "songs2=get_n_songs(data_youtube100,n=None) # We repeat this for a much larger n\n",
    "\n",
    "songs_2 = get_n_songs(data_youtube100,n=None)\n",
    "data2 = find_songs(songs_2,mode='expectation')\n",
    "figure2 = go.Figure()\n",
    "draw_linechart(figure2,data2) # but do not plot the trendline per song...\n",
    "trendline2 = np.poly1d(np.polyfit(data_youtube100.likes,data_youtube100.views,1)) # ...getting one from all songs instead.\n",
    "figure2.add_trace(go.Scatter(x=songs_2.likes,y=trendline2(songs_2.likes),line={'color':'black','dash':'dash','width':3},name='Population trend line'))\n",
    "figure2.update_layout(showlegend=False,\n",
    "    title='Views vs likes for several songs (one trendline)',\n",
    "    xaxis_title='Likes',\n",
    "    yaxis_title='views')\n",
    "figure2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of plots\n",
    "\n",
    "The second plot is interesting to look at, but does not show the behavior we wanted to see. The first one does show the behavior we expected: the shared expectation (amount of likes) influences the realization (view count). If the data ever gets too far above or below the trend line (which represents the video's average views per like, its intrinsic quality), there will be a correction, and the data will approach the trend lineagain. This behavior is similar to the behavior in figure 17.9, except for the fact that our expectation and realization can only move upwards.\n",
    "\n",
    "This can also be seen as a model for f(x): if the amount of people who view the song (amount of consumers) goes up, the amount of people who like it (benefit of the good due to network effects) goes up, and vice versa. This would make more sense with a plot that has views on the x-axis and likes on the y-axis, but the correlation can be seen clearly in this plot as well. The current axis distribution was chosen to find a realization-expectation plot, where the amount of likes is the expectation. This choice made sense for what we were trying to find, but is not ideal for finding f(x).\n",
    "# TODO @Emiel jij zei iets over: slope van trendline is hoeveel mensen willing zijn te kopen? Kun je dat hier uitleggen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 - Popularity effects\n",
    "\n",
    "include at least:\n",
    "- plot\n",
    "- code explanation\n",
    "\n",
    "Study the material in Chapter 18. We investigate whether rich-get-richer phenomenon explains the dynamics of the number of views.\n",
    "\n",
    "*Plot the distribution of the number of views among the songs on several different days. Do you observe power laws?*\n",
    "\n",
    "For this, we will add show 3 plots, with youtube data.\n",
    "\n",
    "We start with a linear plot of the data, with the trendline from the lin-log plot.\n",
    "\n",
    "Next we show a log-lin plot of the data, with a trendline calculated. Straight lines in log-lin plots show exponential relations.\n",
    "We also added the trendline from the log-log plot (dashed) to illustrate the difference.\n",
    "(We do not show the trendline created from the log-log plot in the linear plot, because of the enormous difference in scale this causes)\n",
    "\n",
    "Finally, we show a log-log plot of the data, with a trendline calculated for it (dashed line). Straight lines in log-log plots show power law relationships.\n",
    "We also added the trendline from the lin-log plot to illustrate the difference (non-dashed).\n",
    "\n",
    "\n",
    "\n",
    "We then repeat this process for Spotify data to see if there are any interesting differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dates = [u'2015-11-09', u'2016-04-13',u'2016-09-16'] # list of dates that we will use for plotting\n",
    "colors=['tomato','royalblue','gold'] # List of colors that we will use\n",
    "\n",
    "all_data = [data_youtube100,data_spotify100] #List of data sets that we will use\n",
    "all_data_names = ['Youtube data', 'Spotify data'] # Names of those sets\n",
    "metrics = ['View count', 'Popularity'] # Metrics used for those sets\n",
    "for i in range(len(all_data)): # Loop through data sets\n",
    "    \n",
    "    #Set variables\n",
    "    dataset= all_data[i]\n",
    "    dataset_name = all_data_names[i]\n",
    "    metric = metrics[i]\n",
    "    \n",
    "    print '\\n\\n' + dataset_name + '\\n\\n' # Print that we are working on this data set\n",
    "    \n",
    "    # Prepare figures, called day1_fig because they were originally intended do be used for only one day\n",
    "    day1_fig = go.Figure()\n",
    "    day1_fig_log = go.Figure() # linlog plot\n",
    "    day1_fig_loglog = go.Figure() #loglog plot\n",
    "    for date in dates: # Loop through the dates\n",
    "        try:\n",
    "            # If spotify data: data is the view count\n",
    "            data = sorted(dataset[dataset.timestamp == date].views,reverse=True)\n",
    "        except:\n",
    "            # if spotify data: data is the popularity\n",
    "            data = np.array(sorted(dataset[dataset.timestamp == date].popularity,reverse=True))+1 # add 1 to avoid divide by zero errors in popularity\n",
    "\n",
    "        color = colors[dates.index(date)] # Set color for plot\n",
    "        x = np.array(range(len(data))) + 1 # Array with same size as data, starting from 1 to avoid pesky divide-by-zero errors\n",
    "        \n",
    "        # Calculate trend lines (and their residuals)\n",
    "        polynomial_coefficients_log, residuals_log = np.polyfit(x,np.log(data), 1,full=True)[:2] # Returns coefficients of a polynomial of degree 1 (just a linear relation) with least square fit to data\n",
    "        polynomial_coefficients_loglog, residuals_loglog = np.polyfit(np.log(x),np.log(data), 1,full=True)[:2] # Returns coefficients of a polynomial of degree 1 (just a linear relation) with least square fit to data\n",
    "\n",
    "        print 'Sum of the squares of the fit errors for ' + date + ' lin-log trendline: ' + str(round(residuals_log[0],2))\n",
    "        print 'Sum of the squares of the fit errors for ' + date + ' log-log trendline: ' + str(round(residuals_loglog[0],2))\n",
    "\n",
    "        # Give these trend lines human-readable names\n",
    "        polynomial_name_log = 'Exponential fit: e^(' + str(round(polynomial_coefficients_log[0],2)) + 'x) + e^' + str(round(polynomial_coefficients_log[1],2))\n",
    "        polynomial_name_loglog = 'Power law fit: e^' + str(round(polynomial_coefficients_loglog[1],2)) + ' * x^' + str(round(polynomial_coefficients_loglog[0],2))\n",
    "        \n",
    "        # Actually create polynomials from these\n",
    "        polynomial_log = np.poly1d(polynomial_coefficients_log) # Generate a polynomial from these coefficients\n",
    "        polynomial_loglog = np.poly1d(polynomial_coefficients_loglog) # Generate a polynomial from these coefficients\n",
    "\n",
    "        date_name = dataset_name + ' from ' + date # Human-readable name used for data in legend\n",
    "\n",
    "        # Plot all the data!\n",
    "        day1_fig.add_trace(go.Scatter(x=x,y=data,mode='markers',marker={'color': color,'size':3},name=date_name,legendgroup=date))\n",
    "        day1_fig.add_trace(go.Scatter(x=x,y=np.exp(polynomial_log(x)),marker={'color': color},line={'width' : 0.5},name=polynomial_name_log,legendgroup=date))\n",
    "\n",
    "        day1_fig_log.add_trace(go.Scatter(x=x,y=data,mode='markers',marker={'color': color,'size': 3},name=date_name,legendgroup=date))\n",
    "        day1_fig_log.add_trace(go.Scatter(x=x,y=np.exp(polynomial_log(x)),marker={'color': color},line={'width' : 0.5},name=polynomial_name_log,legendgroup=date))\n",
    "        day1_fig_log.add_trace(go.Scatter(x=x,y=np.exp(polynomial_loglog(np.log(x))),line={'dash': 'dashdot','color': color,'width':1.5},name=polynomial_name_loglog,legendgroup=date))\n",
    "\n",
    "\n",
    "        day1_fig_loglog.add_trace(go.Scatter(x=x,y=data,mode='markers',marker={'color': color,'size':3},name=date_name,legendgroup=date))\n",
    "        day1_fig_loglog.add_trace(go.Scatter(x=x,y=np.exp(polynomial_log(x)),line={'color': color,'width' : 0.5},name=polynomial_name_log,legendgroup=date))\n",
    "        day1_fig_loglog.add_trace(go.Scatter(x=x,y=np.exp(polynomial_loglog(np.log(x))),line={'dash': 'dashdot','color': color,'width':1.5},name=polynomial_name_loglog,legendgroup=date))\n",
    "\n",
    "\n",
    "    # Update the figure layouts, title, etc\n",
    "        \n",
    "    day1_fig.update_layout(title=dataset_name + ': Linear plot for song ' + metric + ' vs ranking',\n",
    "                        xaxis_title='Sorted ranking by '+metric+' of song on that day',\n",
    "                        yaxis_title=metric+' of song')\n",
    "    day1_fig_log.update_layout(title=dataset_name + ': Log-lin plot for song '+metric+' vs ranking',\n",
    "                        xaxis_title='Sorted ranking by '+metric+' of song on that day',\n",
    "                        yaxis_title=metric+' of song',\n",
    "                        yaxis_type='log')\n",
    "    day1_fig_loglog.update_layout(title=dataset_name + ': Log-log plot for song '+metric+' vs ranking',\n",
    "                        xaxis_title='Sorted ranking by '+metric+' of song on that day',\n",
    "                        yaxis_title=metric+' of song',\n",
    "                        xaxis_type='log',\n",
    "                        yaxis_type='log')\n",
    "    \n",
    "    # Show plots\n",
    "    \n",
    "    day1_fig.show()\n",
    "    day1_fig_log.show()\n",
    "    day1_fig_loglog.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of plots:\n",
    "\n",
    "The fit of the lin-log plot seems to be better than the fit on the log-log plot. This, combined with the large amount of outliers for the trendlines on the log-log plot, seems to suggest that the distribution might be an exponential one (of the form $b + a^{cx}$ ) instead of a power law (of the form $b \\cdot x^{-c}$ ). The mean squared error (printed above the plots) for the lin-log trendline is far lower than the error for the log-log trendline.\n",
    "\n",
    "While we believe that an exponential relationship is more likely than a power law, one can also see that the trendline for a power-law is is a barely acceptable fit for the data, with c ranging between 1.5 and 2 (quite a weak power law).\n",
    "\n",
    "\n",
    "\n",
    "Doing the same for the Spotify data was an afterthought, as the exercise mentioned 'views', thus suggesting we should only do this for YouTube data. It is ovious that the trend here is linear; the lin-log trendline uses a very small slope which creates a linear fit. This is quite interesting.\n",
    "The way Spotify categorizes their data seems to be made to compensate for the effects of power laws, so that popularity is linearly distributed accross the songs. This may be done to create a more intuitive, huuman-readable number instead of the enormous view counts of popular YouTube videos, which are hard to imagine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 (continued)\n",
    "\n",
    "*Assume that the number of views in the next day is proportional to the total number of views up to the day before. Argue that in this case the number of views will grow exponentially in time.*\n",
    "\n",
    "This is quite easy to prove. Let's say the amount of views of song $i$, $v_i$, is multiplied by some constant $x_i$ for that song, each day.\n",
    "\n",
    "After $t$ days, the amount of views is:\n",
    "\n",
    "$v_i (t) = v_{i_{initial}} \\cdot {x_i}^t $\n",
    "\n",
    "There is an obvious exponential correlation between time and the number of views.\n",
    "\n",
    "*Look again at the plots for the number of views over time that you produced in assignment 2. Do we observe exponential growth on data? Maybe we observe exponential growth at least some periods of time? Do you think we observe the rich get richer phenomenon?*\n",
    "\n",
    "We do not observe exponential growth in our data. Instead, we see trends that resemble linear growth (for songs that are not popular) or logarithmic growth (for songs that are popular). Popular songs that are just entering the mainstream grow really quickly for a short time, sometimes doubling their popularity overnight. This could definitely be seen as exponential growth for a short time, with $x_i=2$.\n",
    "\n",
    "If the amount of people listening to (Western, English-language) songs were to be infinite, we might see exponential growth. However, because the cap of the population is reached quite quickly, we do not observe exponential growth for long periods of time.\n",
    "\n",
    "We can observe the rich get richer phenomenon easily: popular songs (in the top 100) have quick growth, which stabilizes after a while. New, non-popular songs (alarmschijf, megahit) have slower, linear growth (unless they were already popular).\n",
    "\n",
    "*Compare the ranking of songs in the Spotify data set (based on their position in the top-100) with the ranking of songs in the YouTube data set (based on their number of views) over time. Are the outcomes in line? Why (not)?*\n",
    "\n",
    "We already did this before (in the previous cell, where we explain the plots in which we try to find power laws, and in exercise 2, where we comment on the growth over time). However, we will quickly summarize our findings here.\n",
    "\n",
    "View counts for YouTube have a clear exponential or power law distribution, thus the plots are highly skewed. A small percentage of videos hold a very large share of the views. If a video is unpopular, its growth over time is linear, but if a video is popular we will see a very fast growth which then subsides (like a logarithmic function).\n",
    "\n",
    "Spotify popularity is linearly distributed. The most popular song has a ranking of 100, and the 200 songs after that are mostly distributed in a popularity range of 80-100. Thus, there are no power laws here. Growth over time for Spotify popularity seems to always be quite fast, and then slow, like a logarithmic function. This is similar to the correlation seen in popular YouTube videos, but it is much faster here. We do not have any data for unpopular Spotify songs to compare this to, and we do not know how Spotify calculates their popularity ranking, thus drawing meaningful conclusions from this is difficult. However, it is safe to say that the outcomes are not in line, as the metrics are totally different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4 - Information diffusion\n",
    "\n",
    "Read Chapter 19. Do you think the model of information diffusion applies for music preferences? How can you observe this on the data? Is it related to other phenomena discussed above? Again, there is no one right answer to these questions, try to formulate your own ideas.\n",
    "\n",
    "It is likely that information diffusion applies to listening music. However, probably much less clear than, for example, adopting a technology that gives a large benefit when your connections use it as well. Factors that may make the effect less pronounced are:\n",
    "* Music-preference comes from a personal taste\n",
    "* Radio stations and popular playlists prescribe what will be popular\n",
    "* There is a whole lot of music out there\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "On the other hand:\n",
    "* For trends in music the effect may become clearer when looking at artists or (sub)genres isntead of single songs. \n",
    "* A preference in music might change, this can be influenced by a persons environemt.\n",
    "* Music quickly spreads through a network, if your friends like a song they might recommend it to you.\n",
    "* Music is a common conversational topic, so listening to certain artists that your network listens to gives you a benefit of fitting in.\n",
    "* It occurs fairly frequently that tightly knit groups listen to the same genre of music or start liking the same genre over time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 5 - Create your own data\n",
    "\n",
    "include at least:\n",
    "- plot\n",
    "- code explanation\n",
    "\n",
    "Use the YouTube API to collect your own data set to investigate distribution of popularity and the long tail phenomenon. Start with obtaining the number of views of a song of your choice. Make a random selection from the recommendations that come with this song, and obtain the number of views of this recommended song. Repeat this sequence at least 100 times. Does this data set illustrate the strong variation in the market share of different songs (stronger than would be expected on basis of a normal distribution)? Can you observe the long tail in the distribution of popularity? Visualize the distribution with a graph similar to Figure 18.4.\n",
    "\n",
    "A document with instructions on how to use the YouTube API is available on Blackboard. The YouTube API will return results in JSON format, so your data set will be a JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 6 - Conclusions\n",
    "\n",
    "Make conclusions: 1) which effects and models explain best the data on music preferences, 2) which data we need to collect if we want to investigate cascading, network and rich-get-richer effects in music preferences?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
