{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Science Project Week 3 on Network Dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emiel Steegh   - s1846388  \n",
    "Freek Nijweide - s1857746"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "For this week's assignments, we will examine [...]\n",
    "\n",
    "We use several packages this week,please make sure these are installed when trying to run our code:\n",
    "- networkx\n",
    "- numpy\n",
    "- plotly\n",
    "- pygraphviz\n",
    "\n",
    "We decided to use plotly instead of matplotlib, due to [...]\n",
    "\n",
    "its vastly superior 3D plotting capabilities, and its interactivity. We hoped that this would increase the legibility of our results, and we think that it has done so. We came to various conclusions for assignment 3 that we could not have reached without the assistance of interactive 3D plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The following includes are needed to work with graphs and display solutions.\n",
    "from __future__ import division\n",
    "import networkx as nx\n",
    "\n",
    "from IPython.display import SVG\n",
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "import StringIO\n",
    "\n",
    "from networkx.drawing.nx_pydot import read_dot\n",
    "from networkx.drawing.nx_pydot import from_pydot\n",
    "from networkx.drawing.nx_agraph import to_agraph\n",
    "import pydot\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#wacky imports die emiel denkt nodig te hebben\n",
    "import glob #om file paths en zo te fixen\n",
    "import pprint #debugging purposes\n",
    "import pandas as pd #dataframes en zo voor plotly\n",
    "import os #path handling\n",
    "import json #het lezen van die files\n",
    "#coolio\n",
    "\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "except:\n",
    "    print(\"\\n\"+\n",
    "          \"┌---------------- (!) Warning (!) ----------------┐\")\n",
    "    print(\"|                                                 |\")\n",
    "    print(\"| \\x1b[31m  It looks like Plotly could not be imported,\\x1b[0m   |\")\n",
    "    print(\"|   please make sure it is properly installed.    |\")\n",
    "    print(\"|                                                 |\")\n",
    "    print(\"└-------------------------------------------------┘\")\n",
    "else:\n",
    "    print(\"\\x1b[32mimports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#credits for the following function go to Benjamin Toueg\n",
    "#https://stackoverflow.com/questions/16888409/suppress-unicode-prefix-on-strings-when-using-pprint\n",
    "def my_safe_repr(object, context, maxlevels, level):\n",
    "    typ = pprint._type(object)\n",
    "    if typ is unicode:\n",
    "        object = str(object)\n",
    "    return pprint._safe_repr(object, context, maxlevels, level)\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "pp.format = my_safe_repr\n",
    "\n",
    "def ANSI_col_code(color):\n",
    "    code = 31\n",
    "    if type(color) not in [str, float]:\n",
    "        code = 31\n",
    "    elif color.lower() in ['r', \"red\"]:\n",
    "        code = 31\n",
    "    elif color.lower() in ['g', \"green\"]:\n",
    "        code = 32\n",
    "    elif color.lower() in ['b', \"blue\"]:\n",
    "        code = 94\n",
    "    elif color.lower() in ['y', \"yellow\"]:\n",
    "        code = 33\n",
    "    elif color.lower() in ['m', \"magenta\", 'p', \"purple\"]:\n",
    "        code = 35\n",
    "    return code\n",
    "\n",
    "def cprint(string, color = 'r'):\n",
    "    code = ANSI_col_code(color)\n",
    "    print \"\\x1b[{}m{}\\x1b[0m\".format(code, string)\n",
    "\n",
    "def cstring(string, color = 'r'):\n",
    "    code = ANSI_col_code(color)\n",
    "    return \"\\x1b[{}m{}\\x1b[0m\".format(code, string)\n",
    "\n",
    "cprint(\"Red\", 'r')\n",
    "cprint(\"Green\", 'g')\n",
    "cprint(\"Blue\", 'b')\n",
    "cprint(\"Yellow\", 'y')\n",
    "cprint(\"Purple\", 'm')\n",
    "print(\"back to normal\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data read, parse & store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filetype_paths (data_folder, filetype='json'):\n",
    "    '''\n",
    "    returns a list of paths of the *.json files in a dir (data_folder)\n",
    "    '''\n",
    "    target = os.path.join(data_folder, '*.{}'.format(filetype))\n",
    "    paths = glob.glob(target)\n",
    "    return paths\n",
    "\n",
    "def path_to_timestamp(path):\n",
    "    '''\n",
    "    turns yyyymmdd into yyyy-mm-dd\n",
    "    '''\n",
    "    dateraw = ((os.path.split(path)[1])[:8])\n",
    "    date = \"{}-{}-{}\".format(dateraw[:4], dateraw[4:6], dateraw[6:8])\n",
    "    return date\n",
    "\n",
    "\n",
    "def data_from_youtube(folder):\n",
    "    #grab the list of files to parse\n",
    "    ls_locations = filetype_paths(folder, 'json')\n",
    "    \n",
    "    #data frame that will conatain all relevant data\n",
    "    df = pd.DataFrame(columns=['timestamp', 'song', 'youtube_id',  'release_date', 'views', 'votes', 'likes', 'dislikes'])\n",
    "    \n",
    "    #dict conatining the different collumns for appending data\n",
    "    new_row = {'timestamp' : \"_\", 'release_date' : \"_\", 'song' : \"_\", 'youtube_id' : \"_\", 'views' : 0, 'votes' : 0, 'likes' : 0, 'dislikes' : 0}\n",
    "    \n",
    "    #loops through all files in the data folder\n",
    "    for loc in ls_locations:\n",
    "        \n",
    "        #grabs datagather timestamp from file title\n",
    "        new_row['timestamp'] = path_to_timestamp(loc)\n",
    "        file_raw_data =json.load(open(loc))\n",
    "        \n",
    "        #take all the other data from the relevant locations in the file\n",
    "        for entry in file_raw_data:\n",
    "            new_row['song'] = (entry[\"snippet\"])[\"title\"]\n",
    "            new_row['youtube_id'] = entry[\"id\"]\n",
    "            new_row['release_date'] = ((entry[\"snippet\"])[\"publishedAt\"])[:10]\n",
    "            stats = entry[\"statistics\"]\n",
    "            new_row['views'] = int(stats[\"viewCount\"])\n",
    "            new_row['likes'] = int(stats[\"likeCount\"])\n",
    "            new_row['dislikes'] = int(stats[\"dislikeCount\"])\n",
    "            new_row['votes'] = new_row['likes']+new_row['dislikes']\n",
    "            \n",
    "            df = df.append(new_row, ignore_index=True)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def data_from_spotify(folder):\n",
    "    \n",
    "    ls_locations = filetype_paths(folder, 'json')\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['timestamp', 'song', 'spotify_id', 'release_date', 'popularity'])\n",
    "    \n",
    "    new_row = {'timestamp' : \"_\", 'release_date' : \"_\", 'song' : \"_\", 'spotify_id' : \"_\", 'popularity' : 0}\n",
    "    \n",
    "    for loc in ls_locations:\n",
    "        new_row['timestamp'] = path_to_timestamp(loc)\n",
    "        file_raw_data = json.load(open(loc))\n",
    "        \n",
    "        items = (file_raw_data[\"tracks\"])[\"items\"] # the list of song-datas in list format\n",
    "        for item in items:\n",
    "            track = item[\"track\"]\n",
    "            artists = \"\"\n",
    "            for artist in (track[\"artists\"]):\n",
    "                add_artist = (artist[\"name\"]).encode('utf-8')#unicode(artist[\"name\"], 'ascii')\n",
    "                if artists == \"\":\n",
    "                    artists = add_artist\n",
    "                else:\n",
    "                    artists += \", {}\".format(add_artist)\n",
    "                    \n",
    "            new_row['song'] = \"{} - {}\".format(artists, (track[\"name\"]).encode('utf-8'))\n",
    "            new_row['spotify_id'] = track[\"id\"]\n",
    "            new_row['popularity'] = track[\"popularity\"]\n",
    "            new_row['release_date'] = (item[\"added_at\"])[:10]            \n",
    "            \n",
    "            df = df.append(new_row, ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "            \n",
    "    \n",
    "def create_and_store_data():\n",
    "    '''\n",
    "    Wrapper of the read and parse function, stores the desired data from the 4 folders in 4 seperate .csv files\n",
    "    The reading all the data files and wrangling the data into a useful format takes a bunch of your time, we dont want that.\n",
    "    This definition does all that once and stores the created dataframes as csv files in a new folder.\n",
    "    This way the process only has to happen once and on all consecutive runs of the notebook, we just have to read 4 files that contain the data we want and in the way we want it\n",
    "    \n",
    "    please only run it through def check_or_create_data\n",
    "    '''\n",
    "    location = os.path.join('data','spotify_top100') #spotify top 100\n",
    "    df_spotify_top100 = data_from_spotify(location)\n",
    "    df_spotify_top100.to_csv(os.path.join('csv_data', 'wrangled_spotify_top100.csv'), index=False, sep='\\t', encoding='utf-16')\n",
    "    cprint(\"set (1/4) done: spotify\",'y')\n",
    "    #pp.pprint(df_spotify_top100)\n",
    "\n",
    "    location = os.path.join('data','youtube_top100') #youtube top 100\n",
    "    df_youtube_top100 = data_from_youtube(location)\n",
    "    df_youtube_top100.to_csv(os.path.join('csv_data', 'wrangled_youtube_top100.csv'), index=False, sep='\\t', encoding='utf-16')\n",
    "    cprint(\"set (2/4) done: youtube\",'y')\n",
    "    #pp.pprint(df_youtube_top100)\n",
    "\n",
    "    location = os.path.join('data','radio3fm_megahit') #radio 3fm popular track of the day\n",
    "    df_3fm_megahit = data_from_youtube(location)\n",
    "    df_3fm_megahit.to_csv(os.path.join('csv_data', 'wrangled_3fm_megahit.csv'), index=False, sep='\\t', encoding='utf-16' )    \n",
    "    cprint(\"set (3/4) done: radio 3fm\",'y')\n",
    "    #pp.pprint(df_3fm_megahit)\n",
    "\n",
    "    location = os.path.join('data','radio538_alarmschijf') #radio 538 tracks that may become popular??\n",
    "    df_538_alarmschijf  = data_from_youtube(location)\n",
    "    df_538_alarmschijf.to_csv(os.path.join('csv_data', 'wrangled_538_alarmschijf.csv'), index=False, sep='\\t', encoding='utf-16')    \n",
    "    cprint(\"set (4/4) done: radio 538\",'y')\n",
    "    #pp.pprint(df_538_alarmschijf)\n",
    "    \n",
    "data_megahit      = 'global'\n",
    "data_alarmschijf  = 'global'\n",
    "data_youtube100   = 'global'\n",
    "data_spotify100   = 'global'\n",
    "\n",
    "\n",
    "def check_or_create_data():\n",
    "    '''\n",
    "    Checks wether the required data files are present, if they are not runs the func to create them\n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists('csv_data'):\n",
    "        os.mkdir('csv_data')\n",
    "    \n",
    "    csv_goal = [os.path.join('csv_data', 'wrangled_3fm_megahit.csv'),\n",
    "           os.path.join('csv_data', 'wrangled_538_alarmschijf.csv'),\n",
    "           os.path.join('csv_data', 'wrangled_youtube_top100.csv'),\n",
    "           os.path.join('csv_data', 'wrangled_spotify_top100.csv')]\n",
    "    csv_present = filetype_paths('csv_data', 'csv')\n",
    "\n",
    "    if sorted(csv_present) == sorted(csv_goal):\n",
    "        cprint(\"all data files seem present!\",'g')\n",
    "    else:\n",
    "        cprint(\"data is missing and will be created, this may take a while\", 'r')\n",
    "        create_and_store_data()\n",
    "        cprint(\"all done!\", 'g')\n",
    "    global data_megahit, data_alarmschijf, data_youtube100, data_spotify100\n",
    "    data_megahit = pd.read_csv(os.path.join('csv_data', 'wrangled_3fm_megahit.csv'), delimiter = '\\t', encoding='utf-16')\n",
    "    data_alarmschijf = pd.read_csv(os.path.join('csv_data', 'wrangled_538_alarmschijf.csv'), delimiter = '\\t', encoding='utf-16')\n",
    "    data_youtube100 = pd.read_csv(os.path.join('csv_data', 'wrangled_youtube_top100.csv'), delimiter = '\\t', encoding='utf-16')\n",
    "    data_spotify100 = pd.read_csv(os.path.join('csv_data', 'wrangled_spotify_top100.csv'), delimiter = '\\t', encoding='utf-16')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "check_or_create_data()\n",
    "\n",
    "testprint = False\n",
    "if testprint:\n",
    "    print \"\\n\\n\"\n",
    "    cprint(\"--- --- Radio3fm Megahit:\",'p')\n",
    "    print data_megahit\n",
    "    cprint(\"--- --- Radio538 Alarmschijf:\",'p')\n",
    "    print data_alarmschijf\n",
    "    cprint(\"--- --- Spotify Top100 Dataset:\",'p')\n",
    "    print data_spotify100\n",
    "    cprint(\"--- --- Youtube Top100 Dataset:\",'p')\n",
    "    print data_youtube100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1 - Cascading effects\n",
    "\n",
    "include:\n",
    "- plot\n",
    "- code explanation\n",
    "\n",
    "Read the material in Chapter 16 on cascading effects. Plot the difference between the number of likes and dislikes for several songs.\n",
    "\n",
    "Do you think we observe cascading effects? Is there a difference between songs that are already popular (in the top-100) and those that are not (megahit or alarmschijf)?. The model in Chapter 16 in its pure form cannot be applied to music preferences. Why not? Can you modify the model accordingly? Can you quantify cascading effects in this setting?\n",
    "\n",
    "There is no one right answer right to this question, and the answer may depend on a particular song. Make your conclusions based on your data and present clear arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#TODO plot likes vs dislikes for several songs categorizeb by popularity\n",
    "#TODO why cant the ch 16 model bot be applied to songs (int vs bool)\n",
    "#TODO change model to work for songs\n",
    "#TODO quantify cascading effect\n",
    "#TODO comment code\n",
    "#TODO pretty graphs\n",
    "\n",
    "def find_songs():\n",
    "    dt_songs = {}\n",
    "    \n",
    "    for index, row in data_youtube100.iterrows():\n",
    "        y_id = row['song']\n",
    "        relative_likes = int(row['likes'])-int(row['dislikes'])\n",
    "        date_data_recorded = row['timestamp']\n",
    "        if y_id not in dt_songs.keys():\n",
    "            dt_songs[y_id] = [[relative_likes],[date_data_recorded]]\n",
    "        else:\n",
    "            dt_songs[y_id][0].append(relative_likes)\n",
    "            dt_songs[y_id][1].append(date_data_recorded)\n",
    "                        \n",
    "    return(dt_songs)\n",
    "     \n",
    "def draw_linechart(dt_data):\n",
    "    fig = go.Figure()\n",
    "    for song, data in dt_data.items():\n",
    "        fig.add_trace(go.Scatter(y=data[0], x=data[1], name=song, mode='lines'))\n",
    "    fig.show()\n",
    "    \n",
    "data = find_songs()\n",
    "draw_linechart(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Network effects\n",
    "\n",
    "include  at least:\n",
    "- plot\n",
    "- code explanation\n",
    "\n",
    "Study the material in Chapter 17. Our goal now is to investigate whether we observe network effects in music preferences. How will you choose the data for this purpose? Which songs will suit most?\n",
    "\n",
    "For several songs of your choice plot the actual number of views against time. Assume that without network effects we can expect that users visit a website with a certain frequency and view the song if it matches their taste. Hence, the expected number of views grows linearly in time. Compare your plot to Figure 17.4. Do you think you observe network effects?\n",
    "\n",
    "How will you interpret the network benefit function f, the intrinsic interest function r, and the price p*? Try to specify the model that best fits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 - Popularity effects\n",
    "\n",
    "include at least:\n",
    "- plot\n",
    "- code explanation\n",
    "\n",
    "Study the material in Chapter 18. We investigate whether rich-get-richer phenomenon explains the dynamics of the number of views.\n",
    "\n",
    "Plot the distribution of the number of views among the songs on several different days. Do you observe power laws?\n",
    "\n",
    "Assume that the number of views in the next day is proportional to the total number of views up to the day before. Argue that in this case the number of views will grow exponentially in time.\n",
    "\n",
    "Look again at the plots for the number of views over time that you produced in assignment 2. Do we observe exponential growth on data? Maybe we observe exponential growth at least some periods of time? Do you think we observe the rich get richer phenomenon?\n",
    "\n",
    "Compare the ranking of songs in the Spotify data set (based on their position in the top-100) with the ranking of songs in the YouTube data set (based on their number of views) over time. Are the outcomes in line? Why (not)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4 - Information diffusion\n",
    "\n",
    "Read Chapter 19. Do you think the model of information diffusion applies for music preferences? How can you observe this on the data? Is it related to other phenomena discussed above? Again, there is no one right answer to these questions, try to formulate your own ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 5 - Create your own data\n",
    "\n",
    "include at least:\n",
    "- plot\n",
    "- code explanation\n",
    "\n",
    "Use the YouTube API to collect your own data set to investigate distribution of popularity and the long tail phenomenon. Start with obtaining the number of views of a song of your choice. Make a random selection from the recommendations that come with this song, and obtain the number of views of this recommended song. Repeat this sequence at least 100 times. Does this data set illustrate the strong variation in the market share of different songs (stronger than would be expected on basis of a normal distribution)? Can you observe the long tail in the distribution of popularity? Visualize the distribution with a graph similar to Figure 18.4.\n",
    "\n",
    "A document with instructions on how to use the YouTube API is available on Blackboard. The YouTube API will return results in JSON format, so your data set will be a JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 6 - Conclusions\n",
    "\n",
    "Make conclusions: 1) which effects and models explain best the data on music preferences, 2) which data we need to collect if we want to investigate cascading, network and rich-get-richer effects in music preferences?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
