{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Science Project Week 2 on Information Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emiel Steegh   - s1846388  \n",
    "Freek Nijweide - s1857746"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "For this week's assignments, we will examine graph structures that are similar to the internet. We will examine how these structures are vulnerable to exploits, and how certain algorithms act upon them.\n",
    "\n",
    "We use several packages this week, which are mostly the same as last week (except for plotly). Please make sure these are installed when trying to run our code:\n",
    "- networkx\n",
    "- numpy\n",
    "- plotly\n",
    "- pygraphviz\n",
    "\n",
    "We decided to use plotly for this week instead of matplotlib, due to its vastly superior 3D plotting capabilities, and its interactivity. We hoped that this would increase the legibility of our results, and we think that it has done so. We came to various conclusions for assignment 3 that we could not have reached without the assistance of interactive 3D plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1 (HOOD): Page ranking factors for the DuckDuckGo.\n",
    "DuckDuckGo (DDG) [1] is a semantic-based search engine. This means that it stores and uses semantic information of words and websites to enhance the search query to include contextual data, as opposed to looking for keywords only. [2] \n",
    "For example, when you look up “Thom Palstra” on DDG you will get the Wikipedia page of the University of Twente as the 10th result, a contextual result for Thom Palstra, the Rector Magnificus at the University of Twente. This can be used to your benefit by trying to play into the intent of the searcher.\n",
    "\n",
    "DDG gives very little information about the way they rank pages, but “Nevertheless, the best way to get good rankings (in pretty much all search engines) is to get links from high-quality sites.” [3]. This lets us know that they use some form of a link-based ranking as their main ranking force. Like most search-engines they highly value OUT-links, pointing from websites in the SCC to your website.\n",
    "\n",
    "Prizing themselves as a ‘privacy search engine’ they do not track or record user information, so they do not use user-context for their semantic search. This means that the results of query should be the same for anyone who enters it, wherever they are. [4]\n",
    "When you include “near me” in a query it will use a rough location based on your IP the founder explains[5]. Because DDG does not use the user’s exact context and data for the search, they will most likely adapt the way they search to get the results they want. For finding locations they will most likely include fairly specific locations as text in their query. This logic applies broadly to how the engine is optimally used, to get better-tailored results the query will have to be more specific. So it is a good idea to let the DuckDuckBot (their web crawler) know precise information. Do not try to target a broad audience but instead define a niche and target that.\n",
    "\n",
    "DDG states that they use over 400 sources[4], the two big names they mention are Verizon Media (used to be Yahoo) and Bing. So it seems like a good idea to optimize your page to the Yahoo and Bing standard[6], [7]. \n",
    "\n",
    "DDG blocks what they believe are spam or low-quality websites from their results.[8] So to rank better, it is advised to keep the content of your website as high quality as possible and avoid creating a site devoid of content or designed for AdSense. Users can report low-quality results they find and this may gravely impact your ranking or even get you blocked.\n",
    "\n",
    "In summary, to optimize your page rank on DuckDuckGo:\n",
    "Get links from prominent sites;\n",
    "Think about the intent of your user and the semantics paired with it;\n",
    "Use specific keywords, DDG users will most likely use more detailed queries, think about what they will look for exactly (especially location-wise);\n",
    "Optimize for Bing and Verizon Media search engines;\n",
    "Keep the (content) quality of your website high and avoid spam-like practices.\n",
    "\n",
    "\n",
    "\n",
    "## Assignment 2 (FAKE): Solving the fake news problem\n",
    "Fake news is a large problem in modern information technology. This is mostly seen on social media sites such as Facebook and Twitter, where users get to see a curated feed of content suited to their tastes, which might contain fake news. However, search engines such as Google can also be affected by this. A page P may achieve high search engine rankings while reporting untruthful information, with or without malicious intent.\n",
    "\n",
    "The current solutions for this are sparse, and do not always work as intended. After the Cambridge Analytica scandal, Facebook has publicly committed to fighting fake news (although they still do not check political ads for how truthful they are[9] ). Their current solution is to ask arbiters from countries around the world to verify the truthfulness of news articles.[10] In the Netherlands, this is done by NU.nl and Nieuwscheckers (an initiative by the University of Leiden)[11]. While this is preferable to the alternative, which is not checking the accuracy of news articles on their platform at all, it is quite a controversial solution.[12] The position of power granted to NU.nl (a journalism website) gives it an unfair advantage towards its competitors, because it has the capability to mark their news as fake, thus bringing down the revenue for competitors' articles.\n",
    "\n",
    "Meanwhile, Google has also pledged to fight fake news[13], [14], although we cannot measure how much this has affected the spread of fake news (because its approach has not been as controversial as Facebook's). Twitter has not publicly made such pledges, but they have been banning more accounts recently for publishing untruthful information, and are planning to ban all political advertising.[15] Their true stance towards preventing fake news is questionable, as Twitter still allows fake news from accounts such as the US President, Donald Trump.\n",
    "\n",
    "We propose several ideas, given unlimited budget and manpower, to solve these problems:\n",
    "\n",
    "The first possible solution is to train an AI (neural network with several other human-defined constraints) to rank articles by their truthfulness, and display this score to the user. This score would then be displayed (in a human-readable format, so \"Untrustworthy\" instead of 0.1245...) next to the article link in the search engine / social media platform. This solution has several advantages:\n",
    "\n",
    "- It is multidimensional. Many different variables can be processed by such an AI, such as the tone of the language used, the popularity of the website itself, its previous truthfulness rankings, constraints added by the developers (\"Blogs are inherently untrustworthy\"), user input on the search engine itself (letting them answer questions \"how trustworthy would you rate this article?\"), etc. Computers are perfectly capable of processing all those factors, while a human would quickly lose sight of the bigger picture when examining these differences.\n",
    "- A computerized solution is much faster than a team of humans. This lets the user instantly access news that was released seconds ago, while already knowing how trustworthy the article is. A team of humans would need at least several minutes to rate the truthfulness of the article, reducing the speed of the news.\n",
    "- Keeping this \"score\" next to the link in the search engine warns the user that an article might be untrustworthy, but does not unfairly affect the revenue stream of false positives. The article will still be at the top of the search results, it may just have a warning next to it.\n",
    "\n",
    "There are also several problems with this idea:\n",
    "- Artificial intelligences are prone to bias. If the creators use a biased dataset to train the AI, it will retain that bias. For example, if the creators are more likely to report a conservative news source as untrustworthy, the AI will also have this tendency.\n",
    "- Training an AI is expensive. A large amount of computing power is needed for this. Furthermore, an incredibly large dataset is required, which needs to be ranked according to several variables by human operators. These need to be paid. Luckily, we have unlimited resources in this assignment.\n",
    "- Not malleable. If a change needs to be made, it is quite hard to retrain an existing neural network, and this may be quite costly.\n",
    "- This solution is prone to errors. Inconsistencies in the model may lead to false positives. If an author uses a certain vocabulary, their article may be flagged as untrustworthy. A team of humans would never do this.\n",
    "\n",
    "The second solution is to train a large team of humans. This solution is similar to the first one.\n",
    "The positive aspects of this are:\n",
    "- Less prone to errors (explained above).\n",
    "- Criteria for trustworthiness can easily be changed over time by simply telling the employees to grade pages differently. Thus, changing this solution over time is very easy and quick\n",
    "\n",
    "Negative aspects:\n",
    "- Much more expensive, as a team of employees now needs to be paid beyond the R&D phase of the project.\n",
    "Humans require more time to rank an article (thus making the delivery of checked news less quick).\n",
    "\n",
    "A third possible solution would be to train an algorithm to aggregate news from different sources, and create a summary.\n",
    "Positive aspects of this include:\n",
    "- This process gives an all-encompassing view of actualities. This is a sum of all different opinions of journalists across the globe on this article. Therefore, this summary is the most likely version of \"neutral\" news one might ever hope to get. Even if it is not truly neutral, it invites the reader to think critically as completely contrasting opinions will be juxtaposed.\n",
    "- Reduces the need for users to read multiple news sources, they have just one source for news. This saves quite some time for avid readers.\n",
    "\n",
    "Problems with this solution:\n",
    "- There is little incentive for journalists to produce content if this system becomes popular. Unless the creators of this program forward a share of the revenue to the journalists they aggregated content from, they will not receive any profits for their work, because people will only visit this system and not the journalists' websites.\n",
    "- A lot of power is concentrated in one location if this system becomes popular. While the creators may have good intentions, future owners of this system may use it for manipulative purposes. Furthermore, by having one very popular news source which is run by an algorithm, the system will become the target of hackers and others trying to exploit it. One does not need to actually hack the system to exploit its vulnerabilities; an idea like Search Engine Optimization could be used, finding the best \"trigger words\" to make the system prioritize one's manipulative content.\n",
    "- This program is unlikely to reach lots of people as they still have their own favorite news sources. They are not likely to switch completely to this content aggregator. Furthermore, it does not solve the problem of fake news being shared on a social media platform. Any articles shared on, for example, Facebook will not be affected by this program, even if it is made by Facebook. A proper way would have to be found to make sure this aggregate version gets seen by users who would otherwise be exposed to fake news.\n",
    "\n",
    "We would choose to implement the first concept. It clearly has the most return on investment (where the return is measured in impact). The second option could be just as effective, but the costs to achieve this are far higher. The third option has lots of potential, but it seems hard to achieve much impact using this system. Maybe somebody with better marketing skills could come up with superior use cases.\n",
    "\n",
    "### References for assignments 1 and 2\n",
    "\n",
    "- [1]\t“DuckDuckGo — Privacy, simplified,” DuckDuckGo. [Online]. Available: https://duckduckgo.com/. [Accessed: 19-Nov-2019].\n",
    "- [2]\tD. V. Parsania, F. Kalyani, and K. Kamani, “A Comparative Analysis: DuckDuckGo Vs. Google Search Engine,” Global Research and Development Journal for Engineering, Jan. 2017.\n",
    "- [3]\tDuckDuckGo, “Rankings (SEO),” DuckDuckGo Help Pages. [Online]. Available: https://help.duckduckgo.com/duckduckgo-help-pages/results/rankings/. [Accessed: 19-Nov-2019].\n",
    "- [4]\tDuckDuckGo, “Sources,” DuckDuckGo Help Pages. [Online]. Available: https://help.duckduckgo.com/duckduckgo-help-pages/results/sources/. [Accessed: 19-Nov-2019].\n",
    "- [5]\t“How does DuckDuckGo know where I am?,” Quora. [Online]. Available: https://www.quora.com/How-does-DuckDuckGo-know-where-I-am. [Accessed: 19-Nov-2019].\n",
    "- [6]\t“Bing - SEO-analyse.” [Online]. Available: https://www.bing.com/toolbox/seo-analyzer. [Accessed: 19-Nov-2019].\n",
    "- [7]\t“How to Optimize for Yahoo!” [Online]. Available: http://www.searchengineguide.com/ross-dunn/how-to-optimize.php. [Accessed: 19-Nov-2019].\n",
    "- [8]\tC. Mims, “The Search Engine Backlash Against ‘Content Mills,’” MIT Technology Review, 26-Jul-2010. [Online]. Available: https://www.technologyreview.com/s/419965/the-search-engine-backlash-against-content-mills/. [Accessed: 19-Nov-2019].\n",
    "- [9]\t“Concerns about Facebook’s political ad policy brought to Zuckerberg's dinner table,” CNN Politics. [Online]. Available: https://edition.cnn.com/2019/11/05/politics/facebook-mark-zuckerberg-political-ads/index.html. [Accessed: 19-Nov-2019].\n",
    "- [10]\t“Working to Stop Misinformation and False News,” Working to Stop Misinformation and False News | Facebook Media. [Online]. Available: https://www.facebook.com/facebookmedia/blog/working-to-stop-misinformation-and-false-news. [Accessed: 22-Nov-2019].\n",
    "- [11]\t“Facebook gaat ook in Nederland nepnieuws aanpakken.” [Online]. Available: https://nos.nl/artikel/2160892-facebook-gaat-ook-in-nederland-nepnieuws-aanpakken.html. [Accessed: 22-Nov-2019].\n",
    "- [12]\tR. Kist, “‘Concurrenten de maat nemen is ongemakkelijk,’” NRC, 12-Sep-2019. [Online]. Available: https://www.nrc.nl/nieuws/2019/09/12/concurrenten-de-maat-nemen-is-ongemakkelijk-a3973207. [Accessed: 22-Nov-2019].\n",
    "- [13]\tK. Roose, “Google Pledges $300 Million to Clean Up False News,” 20-Mar-2018. [Online]. Available: https://www.nytimes.com/2018/03/20/business/media/google-false-news.html. [Accessed: 22-Nov-2019].\n",
    "- [14]\t“Google News Initiative – Google News Initiative,” Google News Initiative. [Online]. Available: https://newsinitiative.withgoogle.com/. [Accessed: 22-Nov-2019].\n",
    "- [15]\tBBC News, “Twitter to ban all political advertising,” BBC News, 31-Oct-2019. [Online]. Available: https://www.bbc.com/news/world-us-canada-50243306. [Accessed: 22-Nov-2019]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Settings for assignments 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The following includes are needed to work with graphs and display solutions.\n",
    "from __future__ import division\n",
    "import networkx as nx\n",
    "\n",
    "from IPython.display import SVG\n",
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "import StringIO\n",
    "from networkx.drawing.nx_pydot import read_dot\n",
    "from networkx.drawing.nx_pydot import from_pydot\n",
    "from networkx.drawing.nx_agraph import to_agraph\n",
    "import pydot\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# from utils import *\n",
    "# from graphs import *\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "print(\"imports done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions from last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All these definitions are copied from last week's canvas files, utils.py and graphs.py\n",
    "def fromDot(s):\n",
    "  P_list = pydot.graph_from_dot_data(s)\n",
    "  return from_pydot(P_list[0])\n",
    "\n",
    "def draw(G, mapping=None, emapping=None):\n",
    "    '''draw graph with node mapping and emapping''' \n",
    "    A=to_agraph(G)\n",
    "    A.graph_attr['overlap']='False'\n",
    "    if mapping:\n",
    "        if isinstance(mapping, dict):\n",
    "            mapping = nM(mapping)\n",
    "        for n in A.nodes():\n",
    "            mapping(n, A.get_node(n))\n",
    "    if emapping:\n",
    "        if isinstance(emapping, dict):\n",
    "            emapping = eM(emapping)\n",
    "        for e in A.edges():\n",
    "            emapping(e, A.get_edge(e[0],e[1]))\n",
    "    A.layout()\n",
    "    output = StringIO.StringIO()\n",
    "    A.draw(output, format='svg')\n",
    "    return SVG(data=output.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions for use in both assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#General code definitions\n",
    "def generate_M_and_v(graph):\n",
    "    # Generates M (adjacency matrix) and v (list of nodes) for any graph\n",
    "    M = nx.to_numpy_matrix(graph) #type: np.matrix\n",
    "    v = list(graph.nodes)\n",
    "    return M,v\n",
    "\n",
    "def pagerank(graph,beta):\n",
    "    # Our implementation of pagerank\n",
    "    M,nodes = generate_M_and_v(graph)\n",
    "    for i in range(len(nodes)):\n",
    "        node=nodes[i]\n",
    "        if graph.out_degree(node) > 0:\n",
    "            # Divide the numbers in the adjacency matrix by their out-degree, the first step of turning this into a transition matrix\n",
    "            M[i] /= graph.out_degree(node)\n",
    "    M=M.T # We were working with adjacency matrix. The transition matrix is a transpose of this, where each\n",
    "    #       number is divided by the out-degree of the node the edge is coming from\n",
    "    original_v=np.ones(len(M),dtype=float)/len(M) # The vector v, for pagerank, is of length n with each element = 1/n\n",
    "    v = np.copy(original_v)\n",
    "    change_was_made = True\n",
    "    while change_was_made:\n",
    "        # do v' = beta*M*v + (1-beta)*original_v until the result no longer changes\n",
    "        previous_v = np.copy(v)\n",
    "        first_term = beta* (np.array(np.dot(M,v)).flatten())\n",
    "        second_term = np.dot((1-beta),original_v)\n",
    "        v = first_term + second_term\n",
    "        change_was_made = ((abs(v-previous_v).max() ) > 0.00000000000001) #This 0.00... number was experimentally chosen. Smaller values seemed not to converge, for some graphs \n",
    "    \n",
    "    return v\n",
    "\n",
    "def wrap_pagerank_in_dict(graph,beta=0.85):\n",
    "    # Wraps the pagerank function's output in a dict\n",
    "    values = pagerank(graph,beta)\n",
    "    keys = list(graph.nodes())\n",
    "    return dict(zip(keys,values))\n",
    "\n",
    "def order_nodes_by_rank(nodes, rank):\n",
    "    # rank is a list of ranking numbers, where the order corresponds to the order of the list \"nodes\"\n",
    "    # an example using pagerank: rank[i] is the pagerank of the node at nodes[i]\n",
    "    order = np.argsort(rank)[::-1] #sort the nodes by rank number in descending order. order[i] = index of node i in this sorted list\n",
    "    sorted_nodes = [nodes[i] for i in order] #Human readable sorted list of nodes\n",
    "    ordinal_rank = [list(order).index(i) for i in range(len(order))] # Ordinal rank as specified in exercise 4\n",
    "    return sorted_nodes,ordinal_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Definitions\n",
    "\n",
    "Necessary for generating the graphs we will use for testing.\n",
    "\n",
    "For the first graph:\n",
    "The displayed graph shows the generated graph with\n",
    "- The target page (T) in red\n",
    "- 5 supporting pages (f) in beige\n",
    "- 150 inaccesible pages (n) in white\n",
    "- of which 6 became accessible pages (P) in yellow\n",
    "\n",
    "The first graph is created by repeatedly adding nodes n to an initially empty network. When a node is added it will link out to and get links in from existing nodes. for this assignment we chose 3 out & 0 in. A pageranking is then generated for this component. T is added to the graph after and P accessible pages are chosen based on their pageranking percentile to link to T. Finally f support pages are added and connected to and from T.\n",
    "\n",
    "The second graph was merely created to test our ideas in a fixed setting,\n",
    "\n",
    "The graphs after that are copied from the text of assignment 4 on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_network(n_count, out_d_count, in_d_count):\n",
    "    '''\n",
    "    Generates a network around T (Target) with\n",
    "    n inaccesible pages for T\n",
    "    out_d_count amount of out degree per node\n",
    "    in_d_count amount in in degree per node  \n",
    "    '''\n",
    "    \n",
    "    G = nx.DiGraph()   \n",
    "   \n",
    "    for n in range(0, n_count):\n",
    "        new_node = \"n[{}]\".format(n)\n",
    "        G.add_node(new_node)\n",
    "        \n",
    "        nodes_in_network = list(G.nodes())\n",
    "        nodes_in_network.remove(new_node)\n",
    "        \n",
    "        current_node_count = len(nodes_in_network)\n",
    "        \n",
    "        if current_node_count < out_d_count:\n",
    "            nodes_out_index = random.sample(range(current_node_count), current_node_count)\n",
    "        else:\n",
    "            nodes_out_index = random.sample(range(current_node_count), out_d_count)\n",
    "        \n",
    "        if current_node_count < in_d_count:\n",
    "            nodes_in_index = random.sample(range(current_node_count), current_node_count)\n",
    "        else:\n",
    "            nodes_in_index = random.sample(range(current_node_count), in_d_count)\n",
    "        \n",
    "        for index in nodes_out_index:\n",
    "            G.add_edge(new_node, nodes_in_network[index])\n",
    "            \n",
    "        for index in nodes_in_index:\n",
    "            G.add_edge(nodes_in_network[index], new_node)\n",
    "        \n",
    "    return G\n",
    "\n",
    "def generate_test_web():\n",
    "    # Generate a graph we will use in the exercise 4\n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    graph.add_edge('Google', 'Bing')\n",
    "    graph.add_edge('Google', 'Reddit')\n",
    "    graph.add_edge('Reddit', 'Trap')\n",
    "    graph.add_edge('Spider','Trap')\n",
    "    graph.add_edge('Trap','Spider')\n",
    "    graph.add_edge('Reddit', 'Apple')\n",
    "    graph.add_edge('Reddit', 'Wikipedia')\n",
    "    graph.add_edge('Apple', 'Twitter')\n",
    "    graph.add_edge('Twitter', 'Bing')\n",
    "    graph.add_edge('Wikipedia', 'Twitter')\n",
    "    graph.add_edge('Bing', 'Wikipedia')\n",
    "    graph.add_edge('Apple', 'Wikipedia')\n",
    "    graph.add_edge('Bing','Google')\n",
    "    graph.add_edge('Bing','Dead end')\n",
    "    graph.add_edge('IN node','Apple')\n",
    "    graph.name = \"Big web graph (for testing)\"\n",
    "\n",
    "    return graph\n",
    "\n",
    "# The following graphs correspond to the examples in the assignment 4 text on Canvas\n",
    "\n",
    "def gen_arrow():\n",
    "    G = fromDot('''\n",
    "    strict digraph A {\n",
    "    A -> B -> C -> D -> E;\n",
    "    }''')\n",
    "    G.name = \"Arrow graph\"\n",
    "    return nx.DiGraph(G)\n",
    "    \n",
    "def gen_inward():\n",
    "    G = fromDot('''\n",
    "    strict digraph A {\n",
    "    {B C D E F G } -> A;\n",
    "    }''')\n",
    "    G.name = \"Inward graph\"\n",
    "    return nx.DiGraph(G)\n",
    "\n",
    "def gen_lasso():\n",
    "    G = fromDot('''\n",
    "    strict digraph A {\n",
    "    A -> B -> C -> D -> E -> A;\n",
    "    A -> E -> D -> C -> B -> A;\n",
    "    A -> F -> G;\n",
    "    }''')\n",
    "    G.name = \"Lasso graph\"\n",
    "    return nx.DiGraph(G)\n",
    "\n",
    "def gen_grid():\n",
    "    G = fromDot('''\n",
    "    strict digraph A {\n",
    "    A -> {B D};\n",
    "    B -> {C E};\n",
    "    \n",
    "    C -> {F};\n",
    "    D -> {E G};\n",
    "    E -> {F H};\n",
    "    F -> {I};\n",
    "    G -> {H};\n",
    "    H -> {I};\n",
    "    I -> {};\n",
    "    }''')\n",
    "    G.name = \"Grid graph\"\n",
    "    return nx.DiGraph(G)\n",
    "\n",
    "\n",
    "graphs = [\n",
    "generate_test_web(),\n",
    "gen_arrow(),\n",
    "gen_inward(),\n",
    "gen_lasso(),\n",
    "gen_grid()\n",
    "]\n",
    "\n",
    "for graph in graphs:\n",
    "    print graph.name\n",
    "    display(draw(graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we use three dependent variables to plot our data. We want to know the effect of these variables on our target page, T.\n",
    "\n",
    "The first is P, the amount of accessible/incoming pages as defined in  assignment 3. The second is f, the amount of supporting pages as defined in assignment 3. The third is the \"quality\" of the pages P. We define this as the percentile of these pages, when ordering by PageRank. \n",
    "\n",
    "We then calculate the PageRank for these pages again, after connecting T to the pages P. We generate three plots from this. For these plots, and their explanations, regard the cells below.\n",
    "\n",
    "We will deliver a short mathematical proof for why an increasing r (as defined in the exercise text), and an increasing f, would increase the ranking of T.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$n = total\\ amount\\ of\\ nodes$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$F=The\\ set\\ of\\ supporting\\ pages$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$T = the\\ PageRank\\ of\\ target\\ node\\ T$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$r=\\beta\\cdot \\sum_{p} r_p$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$T = r +\\sum_{f \\in F} (net\\ PageRank\\ received\\ from\\ f) $\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$ net\\ PageRank\\ received\\ from\\ f = (1 - \\beta) \\cdot \\frac{1}{n} \\cdot n = 1 - \\beta  $\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$T = r + (1 - \\beta) \\cdot f $\n",
    "\n",
    "Obviously, if r increases, T increases as well. The same holds for f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def return_T_rank(G_o, P_count, quality, f_count, ordered_nodes):\n",
    "    '''\n",
    "    Returns the pagerank of T, the percentile of it's pagerank, and the adjusted version thereof (when leaving supporting pages out of it)\n",
    "    '''\n",
    "    \n",
    "    G = G_o.copy() #a duplicate of the graph to work on\n",
    "    \n",
    "    nodes_count = len(G.nodes())\n",
    "\n",
    "    P_T_connections = [] # a list that will conatain the n pages that will become accessible to T \n",
    "    \n",
    "    index = int(round(quality*nodes_count))\n",
    "    \n",
    "    for P in range(P_count):\n",
    "        P_T_connections.append(ordered_nodes[index-P-1])\n",
    "    \n",
    "    attrs = {}\n",
    "    attrs['T'] = {'style' : 'filled', 'fillcolor' : 'red'}\n",
    "    \n",
    "    G.add_node('T')\n",
    "    \n",
    "    #connect T to all P\n",
    "    for P in P_T_connections:\n",
    "        G.add_edge(P, 'T')\n",
    "        attrs[P] = {'style' : 'filled', 'fillcolor' : 'yellow'}\n",
    "    \n",
    "    #connect T and it's supporting pages\n",
    "    for f in range(int(f_count)):\n",
    "        new_f = \"f[{}]\".format(f)\n",
    "        G.add_edge('T', new_f)\n",
    "        G.add_edge(new_f, 'T')\n",
    "        attrs[new_f] = {'style' : 'filled', 'fillcolor' : '#f5f5dc'}\n",
    "    \n",
    "    drawing = False\n",
    "    if drawing:\n",
    "        nx.set_node_attributes(G, attrs)\n",
    "        display(draw(G))\n",
    "    \n",
    "    pr = pagerank(G, 0.85)\n",
    "    T_pagerank = pr[list(G.nodes).index('T')]\n",
    "    pagerank_order, ordinal = order_nodes_by_rank(list(G.nodes),pr)\n",
    "    T_pagerank_index = pagerank_order.index('T')\n",
    "    reported_percentage = 100 - (T_pagerank_index / len(pr) * 100)\n",
    "    adjusted_percentage = 100 - (T_pagerank_index / (len(pr)-f_count) * 100)\n",
    "    \n",
    "    return T_pagerank, reported_percentage, adjusted_percentage\n",
    "\n",
    "def plot_results(ls_data_in, title_in, q_range,):\n",
    "\n",
    "    l_title = '{} of T based on f and r'.format(title_in)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    itter = 0\n",
    "    for data in ls_data_in:\n",
    "        fig.add_trace(go.Surface(\n",
    "                        visible=False,\n",
    "                        name=\"P = {}\".format(itter),\n",
    "                        z=data, y=100*(1-q_range)))\n",
    "        itter += 1\n",
    "        \n",
    "#     fig.add_trace(contours_z=dict(show=True,\n",
    "#                                         usecolormap=True,\n",
    "#                                         highlightcolor=\"limegreen\",\n",
    "#                                         project_z=True))\n",
    "\n",
    "    fig.update_layout(title=l_title,\n",
    "                    autosize=True,\n",
    "                    scene = dict(\n",
    "                    xaxis_title='amount of supporting pages',\n",
    "                    yaxis_title='percentile of pagerank gained from accessible pages',\n",
    "                    zaxis_title=title_in))\n",
    "    \n",
    "    fig.data[0].visible = True\n",
    "    \n",
    "    #code for sliders from slider example: https://plot.ly/python/sliders/\n",
    "    steps = []\n",
    "    for i in range(len(fig.data)):\n",
    "        step = dict(\n",
    "            method=\"restyle\",\n",
    "            args=[\"visible\", [False] * len(fig.data)],\n",
    "        )\n",
    "        step[\"args\"][1][i] = True  # Toggle i'th trace to \"visible\"\n",
    "        steps.append(step)\n",
    "\n",
    "    sliders = [dict(\n",
    "        active=10,\n",
    "        currentvalue={\"prefix\": \"Frequency: \"},\n",
    "        pad={\"t\": 50},\n",
    "        steps=steps\n",
    "    )]\n",
    "\n",
    "    fig.update_layout(\n",
    "        sliders=sliders\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "def wrapper(Graph, P_range, q_range, f_range):\n",
    "    nodes_count = len(G.nodes())\n",
    "    max_P_count = int(nodes_count/len(q_range))\n",
    "    P_count = max(P_range)\n",
    "    \n",
    "    if P_count > max_P_count:\n",
    "        print \"The number of accessible pages (P) cannot be larger than {} for the current quality range (q)\".format(max_P_count)\n",
    "        return None\n",
    "    \n",
    "    ranked_nodes = pagerank(G, .85)\n",
    "    ordered_nodes, ordinal_rank = order_nodes_by_rank(list(G.nodes),ranked_nodes)\n",
    "\n",
    "    ls_results_pagerank = []\n",
    "    ls_results_percentile = []\n",
    "    ls_results_percentile_adj = []\n",
    "    \n",
    "    for index_P in range(len(P_range)):\n",
    "        print \"working on P = {}\".format(index_P)\n",
    "        \n",
    "        results_pagerank = np.zeros([len(q_range),len(f_range)])\n",
    "        results_percentile = np.zeros([len(q_range),len(f_range)])\n",
    "        results_percentile_adj = np.zeros([len(q_range),len(f_range)])\n",
    "\n",
    "        for index_f in range(len(f_range)):\n",
    "            for index_q in range(len(q_range)):\n",
    "                f_count = f_range[index_f]\n",
    "                quality = q_range[index_q]\n",
    "                T_pagerank, T_percentile, T_percentile_adj = return_T_rank(G, index_P, quality, f_count, ordered_nodes)\n",
    "                results_pagerank[index_q][index_f] = T_pagerank\n",
    "                results_percentile[index_q][index_f] = T_percentile\n",
    "                results_percentile_adj[index_q][index_f] = T_percentile_adj\n",
    "        \n",
    "        ls_results_pagerank.append(results_pagerank)\n",
    "        ls_results_percentile.append(results_percentile)\n",
    "        ls_results_percentile_adj.append(results_percentile_adj)\n",
    "    \n",
    "    plot_results(ls_results_pagerank, \"Pagerank\", q_range)\n",
    "    plot_results(ls_results_percentile, \"Pagerank percentile \",q_range)\n",
    "    plot_results(ls_results_percentile_adj, \"Adjusted pagerank percentile\",q_range)\n",
    "    \n",
    "    \n",
    "    return None\n",
    "            \n",
    "G = generate_network(200, 3, 0)\n",
    "P_range = np.linspace(0,6,7)\n",
    "q_range = np.linspace(0.1,1,10)\n",
    "f_range = np.linspace(0,10,11)\n",
    "#f_range = np.linspace(0,3,4)\n",
    "wrapper(G, P_range, q_range, f_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3 explanation\n",
    "The first plot contains the PageRank: a higher amount of incoming pages (P) increases the importance of f, the amount of supporting pages.\n",
    "There is an obvious linear trend, both the pagerank of P and amount of supporting pages lead to an increase in page rank.\n",
    "Obviously, with P=0, the pagerank of the incoming pages makes no difference.\n",
    "Still, the performance is quite alarming. The component is disconnected and still, T gets quite a high rank with ease.\n",
    "\n",
    "The percentile in the second plot is quite simple. If the PageRank of T is the highest in the graph, it is in the 100th percentile.\n",
    "If it is the worst, it is the 0th percentile. If it has the exact median of all PageRanks, it is in the 50th percentile.\n",
    "The pagerank of the incoming, accessible pages makes no difference here if p=0, like in the previous example.\n",
    "There is a strong trend between T's pagerank percentile and the amount of supporting pages. With only a few pages, it is already the page with the highest PageRank. It is quite astonishing how easy we have exploited the PageRank algorithm.\n",
    "The importance of the incoming PageRank is increased with higher P, but still, for T's PageRank percentile it does not matter much. The amount of supporting pages is the most important factor here. With only one supporting page, T is already in the top 10%.\n",
    "\n",
    "The third plot contains the adjusted PageRank percentile. This should be lower than the original calculated percentile. We now divide by n-(amount of supporting pages) instead of dividing by n to get the percentile. Obviously, when n=10 we add 40 supporting pages, we will automatically be in the top 20%, because 10/50 = 20%.\n",
    "already even if we are still at spot 10 when we order by PageRank. When diving by n-(amount of supporting pages), we get: 10/10 = 100%. Thus, our page would be the 0th percentile.\n",
    "However, it appears this does not make much difference, when comparing this plot to the previous one. Thus, we know the massive increase in percentile is not simply due to the amount of pages added.\n",
    "\n",
    "Thus, it is quite clear each variable leads to an increase in the ranking of T. However, when looking at the percentile of T's rank, f (the amount of supporting pages) is clearly the dominant factor. This is an obvious exploit in PageRank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "\n",
    "There are three different ranking systems discussed in this week's chapters and lectures.\n",
    "The first is an inDegree-based ranking system. The ranking of a page depends on how many pages link to it.\n",
    "This system is easily fooled, and dead-end pages receive incredibly high rankings.\n",
    "\n",
    "The second ranking system discussed is Google's PageRank. This system expands upon the idea of inDegree rankings,\n",
    "by having a node \"spread\" its incoming ranking over its outgoing links. This can also include a degree of randomness,\n",
    "where a node will spread some of its incoming ranking over *all* nods (to avoid spider traps and dead ends).\n",
    "\n",
    "The third ranking system discussed HITS, based on the hub-authority model. In this model, a page is considered a good hub\n",
    "if it links to good authorities (pages with good content that people want to see). A page is considered a good\n",
    "authority if it is linked to by proper hubs. The page's authority ranking is the one actually used in the search.\n",
    "\n",
    "The equations used for HITS are quite simple:\n",
    "\n",
    "$$\n",
    "h' = \\frac{Ma}{max(h')}\n",
    "$$\n",
    "$$\n",
    "a' = \\frac{M^T h}{ max(a')}\n",
    "$$\n",
    "\n",
    "The equation used for PageRank:\n",
    "\n",
    "$$\n",
    "v' = \\beta M v + (1-\\beta) \\begin{bmatrix}\n",
    "1/n \\\\\n",
    "1/n \\\\\n",
    "1/n \\\\\n",
    "1/n \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Note: the pagerank functions can be found at the top of the file, as they were needed in assignment 3 already\n",
    "def generate_indegree_score(graph):\n",
    "    # Returns a list of in-degree score per node.\n",
    "    in_degrees = []\n",
    "    for node in graph.nodes:\n",
    "        if type(graph) == nx.classes.digraph.DiGraph:\n",
    "            in_degrees.append(graph.in_degree(node))\n",
    "        else:\n",
    "            in_degrees.append(graph.degree(node)) # To make this method not fail for undirected graphs\n",
    "    return in_degrees\n",
    "\n",
    "def ordinal_difference(a, b):\n",
    "    # calculate the difference between ordinal rankings, as defined in the text for assignment 4 on Canvas\n",
    "    if type(a) == list:\n",
    "        a = np.array(a,dtype=float)\n",
    "    if type(b) == list:\n",
    "        b = np.array(b,dtype=float)\n",
    "    return sum(abs(a-b))\n",
    "\n",
    "def hits(M):\n",
    "    # Our implementation of HITS\n",
    "    if type(M) != np.matrix:\n",
    "        # If the user accidentally uses a graph as input, convert to adjacency matrix\n",
    "        M = nx.adjacency_matrix(M)\n",
    "    v=np.ones(len(M),dtype=float) # generate v: a vector ones with length n, as per the definition in the slides\n",
    "    change_was_made = True\n",
    "    h=np.copy(v)\n",
    "    a=np.copy(v)\n",
    "    while change_was_made:\n",
    "        # M remains unchanged\n",
    "        previous_a = np.copy(a)\n",
    "        previous_h = np.copy(h)\n",
    "        \n",
    "        a = np.array(np.dot(M.transpose(),h)).flatten() # a' = M^T * h * 1/max(a') as per the definition\n",
    "        a /= a.max()\n",
    "\n",
    "        h = np.array(np.dot(M,a)).flatten() # h' = M * a * 1/max(h') as per the definition\n",
    "        h /= h.max()\n",
    "        \n",
    "        # Stop once the values don't change anymore\n",
    "        change_was_made = ((abs(a-previous_a).max() ) > 0) or ((abs(h-previous_h).max() ) > 0) \n",
    "    \n",
    "    return a,h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prints = False # turn this to true to enable printing of more info!\n",
    "\n",
    "graphs = [generate_test_web(), gen_lasso(), gen_arrow(),gen_inward(),gen_grid()] # Use these graphs in the exercise\n",
    "differences=[]\n",
    "for graph in graphs:\n",
    "    M,v = generate_M_and_v(graph)\n",
    "    if prints:\n",
    "        print graph.name\n",
    "        print str(v) + \"\\n\"\n",
    "    \n",
    "    in_degrees = generate_indegree_score(graph)\n",
    "    human_readable, indegree_order = order_nodes_by_rank(v, in_degrees)\n",
    "    if prints:\n",
    "        print \"Solution to in_degree ranking:\"\n",
    "        print \"Human readable ranking (nodes sorted by their rank): \" + str(human_readable)\n",
    "        print \"Ordinal ranking: \" + str(indegree_order) + \"\\n\"\n",
    "\n",
    "    a , h = hits(M)\n",
    "    human_readable, hits_order = order_nodes_by_rank(v, a)\n",
    "    if prints:\n",
    "        print \"Our solution to HITS ranking\"\n",
    "        print \"Human readable ranking: \" + str(human_readable)\n",
    "        print \"Ordinal ranking: \" + str(hits_order) + \"\\n\"\n",
    "\n",
    "        print \"HITS ranking (networkx implementation, for reference)\"\n",
    "    (real_h_dict, real_a_dict) = nx.hits(graph,max_iter=100000)\n",
    "    difference = []\n",
    "    real_a=[]\n",
    "    for i in range(len(v)):\n",
    "        node = v[i]\n",
    "        real_a.append(real_a_dict[node])\n",
    "        difference.append(abs(real_a_dict[node]-a[i]))\n",
    "    real_human_readable, real_hits_order = order_nodes_by_rank(v, real_a)\n",
    "    if prints:\n",
    "        print \"Human readable ranking: \" + str(real_human_readable)\n",
    "        print \"Ordinal ranking: \" + str(real_hits_order)\n",
    "        print \"Ordinal ranking difference between our implementation and networkx: \" + str (ordinal_difference(hits_order,real_hits_order)) + \"\\n\"\n",
    "\n",
    "    \n",
    "    page_rank_list = pagerank(graph,0.85)\n",
    "    human_readable, pagerank_order = order_nodes_by_rank(v, page_rank_list)\n",
    "    if prints:\n",
    "        print \"Our solution to pagerank\"\n",
    "        print \"Human readable ranking: \" + str(human_readable)\n",
    "        print \"Ordinal ranking: \" + str(pagerank_order) + \"\\n\"\n",
    "        print \"pagerank (networkx implementation, for reference)\"\n",
    "    (real_v_dict) = nx.pagerank(graph,max_iter=100000)\n",
    "    \n",
    "    difference = []\n",
    "    real_v=[]\n",
    "    for i in range(len(v)):\n",
    "        node = v[i]\n",
    "        real_v.append(real_v_dict[node])\n",
    "        difference.append(abs(real_v_dict[node]-real_v[i]))\n",
    "        \n",
    "    real_human_readable, real_pagerank_order = order_nodes_by_rank(v, real_v)\n",
    "    if prints:\n",
    "        print \"Human readable ranking: \" + str(real_human_readable)\n",
    "        print \"Ordinal ranking: \" + str(real_pagerank_order)\n",
    "        print \"Ordinal ranking difference between our implementation and networkx: \" + str (ordinal_difference(pagerank_order,real_pagerank_order))\n",
    "\n",
    "    differences.append([ordinal_difference(indegree_order,hits_order),\n",
    "                        ordinal_difference(indegree_order,pagerank_order),\n",
    "                        ordinal_difference(hits_order,pagerank_order),\n",
    "                        ordinal_difference(hits_order,real_hits_order),\n",
    "                        ordinal_difference(pagerank_order,real_pagerank_order),\n",
    "                        ])\n",
    "    if prints:\n",
    "        print(\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "graphs=[\"Big (n=10) web\", \"Lasso\", \"Arrow\", \"Inward\", \"Grid\"]\n",
    "indegree_hits_diff = [x[0] for x in differences]\n",
    "indegree_pagerank_diff = [x[1] for x in differences]\n",
    "hits_pagerank_diff = [x[2] for x in differences]\n",
    "hits_ours_vs_networkx_diff = [x[3] for x in differences]\n",
    "pagerank_ours_vs_networkx_diff = [x[4] for x in differences]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='In-degree vs HITS', x=graphs, y=indegree_hits_diff),\n",
    "    go.Bar(name='In-degree vs PageRank', x=graphs, y=indegree_pagerank_diff),\n",
    "    go.Bar(name='HITS vs PageRank', x=graphs, y=hits_pagerank_diff),\n",
    "    go.Bar(name='Our HITS implementation vs networkx\\'', x=graphs, y=hits_ours_vs_networkx_diff),\n",
    "    go.Bar(name='Our PageRank implementation vs networkx\\'', x=graphs, y=pagerank_ours_vs_networkx_diff)  \n",
    "])\n",
    "\n",
    "fig.update_layout(barmode='group')\n",
    "fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation for exercise 4\n",
    "It is clear from this plot that there is no difference between our PageRank implementation and the one in networkx. Therefore we are quite confident that our implementation for PageRank is correct. The bar graph for this is always at 0.\n",
    "\n",
    "There is a slight difference between our HITS algorithm for the \"web\" graph (all graphs can be seen a few cells above), and the implementation used in networkx. It is hard to find an explanation for this. It may be because of different execution counts, where the networkx implementation stops before it converges.\n",
    "\n",
    "The HITS and PageRank seem to have the same ranking for Arrow. However, all algorithms are quite similar there. This makes sense, as the pages just link toward each other in a 1D line.\n",
    "\n",
    "In the inward graph, all algorithms are the same. This makes sense: there is one graph with a high ranking (the middle), and all others have an equal ranking, where the tie is broken depending on which graph comes earlier in the list.\n",
    "\n",
    "The bigger differences are seen in Grid, Lasso, and web. In Grid, the in-degree and PageRank algorithm are most similar. This can be expected, as their implementations are slightly similar. Furthermore, the graphs with a high in-degree here will also have a high PageRank due to the graph structure.\n",
    "\n",
    "In Lasso, In-Degree and HITS are most similar. This, again, makes sense due to the graph structure, where one graph has a large in-degree and is thus a good authority.\n",
    "\n",
    "For the Web graph, it is surprising how divergent the results for HITS and PageRank are. It seems these algorithms produce wildly different results for larger, complex graphs, even though they seem quite similar on a theoretical level. We did not expect this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
